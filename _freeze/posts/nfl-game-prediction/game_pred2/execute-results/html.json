{
  "hash": "065e7b11bb6095db783d35355f65620e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting NFL Game Outcomes in R (part 2)\"\nauthor: \"Sameer Sapre\"\ndate: \"2025-01-27\"\ncategories: [code, analysis] \nimage: \"mw_reg.png\"\nexecute: \n  warning: false\npage-layout: full\n---\n\n\n\n## Intro\n\nHello everyone! In the [first post I shared on modeling NFL game outcomes](https://ssapre2.github.io/posts/nfl-game-prediction/), I shared a simple way to create a NFL game prediction model using  `nflreadR` and `tidymodels`. In this post, I'll share a simple way to improve our model using *cross-validation*.\n\n\nI'll skip the data cleaning portion since it's the same as the previous post. Ideally, I'd package these steps into a function as part of a larger package to spare you the time and make this process neater, repeatable, and editable. However, I'm not feeling very generous today (aka I'm too lazy), so I've copied and pasted the setup code in the hidden cell below.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary('nflreadr')\nlibrary(\"tidyverse\")\nlibrary('pROC')\nlibrary('tidymodels')\n\n# Scrape schedule Results\nload_schedules(seasons = seq(2011,2024)) -> nfl_game_results \n\nnfl_game_results %>%\n  # Remove the upcoming season\n  pivot_longer(cols = c(away_team,home_team),\n               names_to = \"home_away\",\n               values_to = \"team\") %>%\n  mutate(team_score = ifelse(home_away == \"home_team\",yes = home_score,no = away_score),\n         opp_score = ifelse(home_away == \"home_team\", away_score,home_score)) %>%  # sort for cumulative avg\n  arrange(season,week) %>%\n  select(season,game_id,team,team_score,opp_score,week) -> team_games\n\nteam_games %>%\n  arrange(week) %>%\n  group_by(season,team) %>%\n  # For each team's season calculate the cumulative scores for after each week\n  mutate(cumul_score_mean = cummean(team_score),\n          cumul_score_opp = cummean(opp_score),\n          cumul_wins = cumsum(team_score > opp_score),\n          cumul_losses = cumsum(team_score < opp_score),\n          cumul_ties = cumsum(team_score == opp_score),\n         cumul_win_pct = cumul_wins / (cumul_wins + cumul_losses),\n         # Create the lag variable\n         cumul_win_pct_lag_1 = lag(cumul_win_pct,1),\n         cumul_score_lag_1 = lag(cumul_score_mean,1),\n         cumul_opp_lag_1 = lag(cumul_score_opp,1)\n         ) %>%\n  # Non-lag variables leak info\n  select(week,game_id,contains('lag_1')) %>%\n  ungroup() -> cumul_avgs\n\n# Calculate average win pct.\nteam_games %>%\n  group_by(season,team) %>%\n  summarise(wins = sum(team_score > opp_score),\n            losses = sum(team_score < opp_score),\n            ties = sum(team_score == opp_score))%>%\n  ungroup() %>%\n  arrange(season) %>%\n  group_by(team) %>%\n  mutate(win_pct = wins / (wins + losses),\n         lag1_win_pct = lag(win_pct,1)) %>%\n  ungroup() -> team_win_pct\n\n# Load depth charts and injury reports\ndc = load_depth_charts(seq(2011,most_recent_season()))\ninjuries = load_injuries(seq(2011,most_recent_season()))\n\ninjuries %>%\n  filter(report_status == \"Out\") -> out_inj\n\ndc %>% \n  filter(depth_team == 1) -> starters\n\n# Determine roster position of injured players\nstarters %>%\n  select(-c(last_name,first_name,position,full_name)) %>%\n  inner_join(out_inj, by = c('season','club_code' = 'team','gsis_id','game_type','week')) -> injured_starters\n\n# Number of injuries by position\ninjured_starters %>%\n  group_by(season,club_code,week,position) %>%\n  summarise(starters_injured = n()) %>%\n  ungroup() %>%\n  pivot_wider(names_from = position, names_prefix = \"injured_\",values_from = starters_injured) -> injuries_position\n\nnfl_game_results %>%\n  inner_join(cumul_avgs, by = c('game_id','season','week','home_team' = 'team')) %>%\n  inner_join(cumul_avgs, by = c('game_id','season','week','away_team' = 'team'),suffix = c('_home','_away'))-> w_avgs\n\n# Check for stragglers\nnfl_game_results %>%\n  anti_join(cumul_avgs, by = c('game_id','season','home_team' = 'team','week')) -> unplayed_games\n\n\n# Indicate whether home team won\nw_avgs %>%\n  mutate(home_win = as.numeric(result > 0)) -> matchups\n\nmatchups %>%\n  left_join(injuries_position,by = c('season','home_team'='club_code','week')) %>%\n  left_join(injuries_position,by = c('season','away_team'='club_code','week'),suffix = c('_home','_away')) %>%\n  mutate(across(starts_with('injured_'), ~replace_na(.x, 0))) -> matchup_full\n\n# Remove unneeded columns\nmatchup_full %>%\n  select(-c(home_score,away_score,overtime,home_team,away_team,away_qb_name,home_qb_name,referee,stadium,home_coach,away_coach,ftn,espn,old_game_id,gsis,nfl_detail_id,pfr,pff,result)) -> matchup_ready\n\n\n# Remove columns\nmatchup_ready = matchup_ready%>%\n  # Transform outcome into factor variable\n  select(where(is.numeric),game_id) %>% \n  mutate(home_win = as.factor(home_win)) \n```\n:::\n\n\nLong story short, I want each row in the final dataset to represent an NFL game with each team's :\n1. Previous performance\n2. Injuries\n3. Opponent's previous performance\n4. Opponent's injuries\n\nThere are definitely better ways to represent the pieces of information than I'm doing above (please let me know!), but I will focus on model tuning as the approach to improving model performance today.\n\nWith that, I'll start directly from the modeling portion of the code. In the sections below, you'll see our basic `tidymodels` setup using our `recipes` object for pre-processing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('tidymodels')\n\n# Split Data\nset.seed(123)\nmatchups24 = matchup_ready %>% filter(season == 2024)\nsplits = matchup_ready %>% \n  filter(season != 2024) %>%\n  initial_split(prop = 0.7)\ntrain_data <- training(splits)\ntest_data  <- testing(splits)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_impute = recipe(formula = home_win ~ .,\n                 data = train_data) %>%\n  #create ID role (do not remove) game ID. We'll use this to match predictions to specific games\n  update_role(game_id, new_role = \"ID\")%>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  #for each numeric variable/feature, replace any NA's with the median value of that variable/feature\n  step_impute_median(all_numeric_predictors()) \n\n# Create recipe to for moneyline model\n```\n:::\n\n\nHere, we can see what the medians (NA-filler) values for each predictor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimp_models <- rec_impute %>%\n  check_missing(all_numeric_predictors()) %>%\n  prep(training = train_data)\n\n# Check the predictors that were filtered out\nnzv_step <- imp_models$steps[[2]]  # Access the step_nzv object\nremoved_predictors <- nzv_step$removals\n\n# Display removed predictors\nremoved_predictors\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNULL\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check if imputation worked\nimp_models %>%\n  bake(train_data) %>%\n  is.na() %>%\n  colSums()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  season                     week                    total \n                       0                        0                        0 \n               away_rest                home_rest           away_moneyline \n                       0                        0                        0 \n          home_moneyline              spread_line         away_spread_odds \n                       0                        0                        0 \n        home_spread_odds               total_line               under_odds \n                       0                        0                        0 \n               over_odds                 div_game                     temp \n                       0                        0                        0 \n                    wind cumul_win_pct_lag_1_home   cumul_score_lag_1_home \n                       0                        0                        0 \n    cumul_opp_lag_1_home cumul_win_pct_lag_1_away   cumul_score_lag_1_away \n                       0                        0                        0 \n    cumul_opp_lag_1_away           injured_S_home          injured_LB_home \n                       0                        0                        0 \n         injured_RB_home           injured_T_home           injured_C_home \n                       0                        0                        0 \n         injured_DT_home          injured_WR_home          injured_CB_home \n                       0                        0                        0 \n          injured_G_home           injured_K_home          injured_TE_home \n                       0                        0                        0 \n         injured_QB_home          injured_DE_home          injured_LS_home \n                       0                        0                        0 \n          injured_P_home          injured_FB_home           injured_S_away \n                       0                        0                        0 \n         injured_LB_away          injured_RB_away           injured_T_away \n                       0                        0                        0 \n          injured_C_away          injured_DT_away          injured_WR_away \n                       0                        0                        0 \n         injured_CB_away           injured_G_away           injured_K_away \n                       0                        0                        0 \n         injured_TE_away          injured_QB_away          injured_DE_away \n                       0                        0                        0 \n         injured_LS_away           injured_P_away          injured_FB_away \n                       0                        0                        0 \n                 game_id                 home_win \n                       0                        0 \n```\n\n\n:::\n\n```{.r .cell-code}\ntidy(imp_models,number = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 0 × 2\n# ℹ 2 variables: terms <chr>, id <chr>\n```\n\n\n:::\n:::\n\n\n\n\n## Modeling \n\nLast time, we did this step without introducing any sort of parameter tuning. This time, we'll try to tune our logistic regression to see if that results in better (and more trustworthy) prediction accuracy. You can see in the `glm_spec` variable we introduce two new lines, `penalty = tune()` and `mixture = tune()`. What does that mean?\n\nIn logistic regression, we have the option of introducing something called **regularization**, which is just a fancy word for telling the model \"don't overcomplicate things!\". As you saw in the feature enigneering steps in the last post, there are a lot of variables and in many real world models (AI models included) the number of parameters can be in the billions! This is often too much information to pare down, so regularization is a simple step to help the model focus on the most important stuff. \nThis extra step helps the model avoid a common ML pitfall - **overfitting**. Think of it like this, imagine you have a math test tomorrow and your friend sends you the solutions to the practice exam. You study by memorizing the answers to practce exam (not by DOING the problems). The next day to open your test and ... shoot... you have to SHOW your work?? You don't even know where to start. That is overfitting, you've memorized the answers, but you don't know HOW to solve the problems or complete the process.\n\nRegularization is a way to penalize the model from memorizing inputs and instead forces it to make generalizations about relationships in the data (aka learn general patterns). You sacrifice a bit accuracy, for example, maybe you mix up some calculations on the exam incorrect, but you get partial credit for showing proper process.\n\nThere are some other pros and cons to regularization, but I'll leave that for you to research yourself.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('glmnet')\n# Penalized Linear Regression\n\n\nglm_spec <- logistic_reg(\n  penalty = tune(),     \n  mixture = tune(),    # Mixutre (0 = Ridge, 1 = Lasso, values in between = Elastic Net)\n    \n) %>%\n  set_engine(\"glmnet\")\n\n# Setup workflow\nglm_wflow <-\n  workflow() %>% \n  add_recipe(rec_impute) %>%\n  add_model(glm_spec)\n\n# Create cross validation splits\nfolds <- vfold_cv(train_data, v = 5)\n\n# Tune the hyperparameters using a grid of values\nglm_tune_results <- tune_grid(\n  glm_wflow,\n  resamples = folds,\n  grid = 10   # Number of tuning combinations to evaluate\n)\n```\n:::\n\n\nNext, let's take a look at the results. We're primarily concerned with **brier score** since we want to measure how well our predicted probabilies are *calibrated* to actual results. Calibration refers to how well the predicted probabilities from a model match the actual observed outcomes. A well-calibrated model means that if it predicts a 70% chance of an event happening, the event should occur about 70% of the time in reality. For example, if a model predicts a team wins 60% of the time, the team should win about 60% of the time.\n\nBrier score is a representation of that calibration on a 0-1 scale. The *lower* the score, the better Seeing a slight reduction in brier score between the last attempt (0.229) and this attempt (0.214) is good! If we want to go the extra mile, we can test the significance of the difference across CV folds to determine if this reduction is significant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Display tuning results\nglm_tune_results %>%\n  collect_metrics() %>%\n  filter(.metric == \"brier_class\") %>%\n  arrange(mean) %>%\n  slice(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 8\n  penalty mixture .metric     .estimator  mean     n std_err .config            \n    <dbl>   <dbl> <chr>       <chr>      <dbl> <int>   <dbl> <chr>              \n1  0.0208   0.185 brier_class binary     0.215     5 0.00432 Preprocessor1_Mode…\n```\n\n\n:::\n:::\n\n\nLooks like the combo of scaling, cross-validation lead us to better model calibration!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select the best hyperparameters based on RMSE\nbest_glm <- select_best(glm_tune_results, metric = 'brier_class')\n\n# Finalize the workflow with the best hyperparameters\nfinal_glm_workflow <- finalize_workflow(glm_wflow, best_glm)\n\nbest_glm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  penalty mixture .config              \n    <dbl>   <dbl> <chr>                \n1  0.0208   0.185 Preprocessor1_Model02\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the finalized model on the entire training data\nfinal_glm_fit <- fit(final_glm_workflow, data =train_data)\n```\n:::\n\n\n\nLet's try out our new evaluation metric to check how well our model is calibrated with the actual output. The graph below is calibration plot. It tells us how aligned our predictions are with the actual observed outcomes. The x-axis is straightforward - it represents our predicted probabilities. The Y-Axis, however, is the proportion of *positive* observations. This displays our **home_win** outcome variable as a proportion so we can compare on the scale of the predicted probability class.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Align predictions to test dataset\npredicted_df = augment(final_glm_fit,test_data) \npredicted_df |> \n  mutate(bin = cut(.pred_1, breaks = seq(0, 1, by = 0.1), include.lowest = TRUE)) %>%\n  group_by(bin) %>%\n  summarise(\n    avg_predicted = mean(.pred_1),\n    observed_proportion = mean(as.numeric(home_win)-1,na.rm = T),\n    n = n()\n  ) |> \n  ggplot(aes(x = avg_predicted, y = observed_proportion)) +\n  geom_point() +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Calibration Plot\",\n    x = \"Mean Predicted Probability\",\n    y = \"Observed Proportion\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](game_pred2_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nIt looks like in this case, the model is better calibrated (in-line with actual results) for games between a 50-70% win pct for the home team. This doesn't tell us much other than that the model is unreliable on the extreme margins, however, NFL games rarely have games with such extreme dogs/favorites. \n\nRegularization and model tuning is just one way to improve model perforance. In the next post, we'll likely take another crack at improving this model. Thank you for reading and feel free to reach out to me with any questions/suggestions!\n\n\n",
    "supporting": [
      "game_pred2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}