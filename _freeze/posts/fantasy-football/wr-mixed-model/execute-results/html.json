{
  "hash": "d440bbeb8e06e570412ff83585ad5420",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting Wide Receiver Fantasy Points w/ Tidymodels Pt 2\"\nsubtitle: \"Random Intercepts\"\nauthor: \"Sameer Sapre\"\ndate: \"2025-05-06\"\ncategories: [code, analysis,tidymodels,nfl]\n#draft: TRUE\neditor: visual\n---\n\n\nI made a slight mistake. I broke a core assumption of linear regression in the previous edition of the model. To use linear regression, one must assume that each data point is **independent** of one another. However, this is not the case in the dataset I was working with. Take a look at the sample below. Here are some of the league leaders in receptions for 2023. We can see clearly that we are dealing with some repeated measurements.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nma_wr %>%\n  filter(season == 2023) %>%\n  group_by(player_id) %>%\n  mutate(season_catches = sum(receptions)) %>%# %>%\n  ungroup() %>%\n  arrange(desc(season_catches)) %>%\n  top_n(170) %>%\n  ggplot(aes(x = week, y = fantasy_points)) + \n  geom_point() + \n  geom_line() +\n  facet_wrap(~ player_name) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSelecting by season_catches\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](wr-mixed-model_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nSeveral records exist for each receiver depicted in the plot. This is an example of what scientists call \"repeated\" measures. It's a way to describe the process of taking several measurements of a variable on the same subject. This is important because it acknowledges that a core assumption of linear regression is violated (independence) since samples taken or observed from the same subject may not be independent.\n\nIn football terms, we want our model to know WHO or WHERE the data is coming from. We want it to know that the data produced by Tyreek Hill is coming from... well... Tyreek Hill. It can be beneficial in this context because it allows us to capture information from our data that may not necessarily be represented by our features (i.e. targets, TDs, yards, etc.)\n\nLet's take the following example scenario: Dolphins starting QB Tua Tagovailoa has been injured for the past few games, but he is back this week. Hill's stats haven't been great with the backup in, but he's STILL Tyreek Hill. While the basic linear model will just be going off recent week stats, it's not aware that it's \\[TYREEK FREAKING HILL\\](<https://www.youtube.com/watch?v=iPHAUaTlH4w>). Acknowledging where the observations come from does that, to an extent.\n\nNow, I'm sure there are probably many nesting structures in the data we're using. When predicting NFL receiving fantasy points, we can probably nest observations within season, week, team, and/or player. For this exercise, we'll focus on using player (player_id in the dataset) as the nesting/grouping variable for interpretability. With player_id as our nesting/grouping variable, let's try to implement this with some familiar code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Build a tidymodels recipe object with fantasy points as the target\nexp_recipe = train_data %>%\n  recipe(formula = fantasy_points ~ .) %>%\n  update_role(c(recent_team,log_fantasy_points,fantasy_points_ppr,fantasy_points_target),new_role = 'ID') %>%\n  add_role(player_id, new_role = \"exp_unit\") %>%\n  # Impute data with median from each column \n  step_impute_median(all_numeric_predictors()) %>%\n # Remove zero variance predictors (ie. variables that contribute nothing to prediction)\n  step_zv(all_predictors()) %>%\n  step_center(all_numeric_predictors()) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the correlation matrix wrt to log fantasy points\ncor_matrix <- train_data %>% \n    select(starts_with(\"wt_\"), fantasy_points) %>%\n    cor(use = \"complete.obs\")\n\ncor_data <- reshape2::melt(cor_matrix)\n\n# Sort variables by correlation value\ncor_data %>% \n  filter(Var1 == \"fantasy_points\") %>% \n  select(-Var1) %>%\n  arrange(desc(value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                             Var2        value\n1                  fantasy_points  1.000000000\n2           wt_fantasy_points_ppr  0.402338725\n3                         wt_wopr  0.396253985\n4              wt_receiving_yards  0.395799999\n5                 wt_target_share  0.394024143\n6        wt_receiving_first_downs  0.389457339\n7                   wt_receptions  0.388656926\n8               wt_fantasy_points  0.383639816\n9                      wt_targets  0.374852224\n10             wt_air_yards_share  0.360107466\n11         wt_receiving_air_yards  0.333653165\n12 wt_receiving_yards_after_catch  0.312735509\n13               wt_receiving_tds  0.253890254\n14               wt_receiving_epa  0.249857025\n15           wt_receiving_fumbles  0.071041547\n16      wt_receiving_fumbles_lost  0.053558232\n17   wt_receiving_2pt_conversions  0.045212262\n18                        wt_racr  0.008076501\n19           wt_special_teams_tds  0.006971226\n20                  wt_depth_team -0.045581061\n```\n\n\n:::\n:::\n\n\nWe're going to train a few simple models, without any regularization, to compare a linear model to a mixed model. This time, we'll use the `workflowsets` library from the `tidymodels` family to combine processes for 2 modeling types (mixed effects and linear models).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Matrix'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'multilevelmod' was built under R version 4.4.3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create CV folds based on player id\nplayer_folds <- group_vfold_cv(\n  train_data,\n  group = player_id,\n  v = 5             \n)\n\n\nlm_spec <- linear_reg() %>% set_engine(\"lm\")\nmixed_mod_spec <- linear_reg() %>%\n  set_engine(\"lmer\")\n\n# Setup workflow with model formula\nmixed_wflow <- workflow() %>% \n  add_recipe(exp_recipe) %>%\n  add_model(spec = mixed_mod_spec, formula = fantasy_points ~ 1 + (1|player_id) +\n              wt_fantasy_points+\n              wt_wopr +\n              wt_receptions \n              )\n\nlm_wflow <- workflow() %>%\n  add_recipe(exp_recipe) %>%\n  add_model(spec = lm_spec,formula = fantasy_points ~ wt_fantasy_points + \n              wt_wopr + wt_receptions)\n\n# Create workflow set for easier comparison\nmodel_set <- as_workflow_set(\n    linear_model = lm_wflow,\n    mixed_model = mixed_wflow\n)\n```\n:::\n\n\nIn the workflow set defined above, we created a simple linear model consisting of the variables *weighted* fantasy points, WOPR, and receptions. I wanted to compare this to the mixed effects model which contained a random intercept that varied by the `player_id`. This is what I mentioned in the section above about taking the player into account in the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cross Validate across workflow sets\ncv_results <- model_set %>%\n  workflow_map(\n    seed = 1502,\n    fn = \"fit_resamples\",\n    resamples = player_folds,\n    metrics = metric_set(rmse, rsq, mae)\n  )\n\n# Collect Metrics and Evaluate Average RMSE across folds\ncollect_metrics(cv_results, summarize = TRUE) %>% \n  filter(.metric == \"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 9\n  wflow_id     .config      preproc model .metric .estimator  mean     n std_err\n  <chr>        <chr>        <chr>   <chr> <chr>   <chr>      <dbl> <int>   <dbl>\n1 linear_model Preprocesso… recipe  line… rmse    standard    5.35     5  0.0679\n2 mixed_model  Preprocesso… recipe  line… rmse    standard    5.43     5  0.0788\n```\n\n\n:::\n:::\n\n\nAfter running the model, the simple linear model actually outperforms the random intercept model according to RMSE. This is not what I expected. Perhaps the nesting structure is incorrect, observations are more independent than they seem, or I have not implemented the mixed model framework correctly. Any way you slice it, we have only really scratched the surface of mixed effect regression models. You can go down a rabbit hole of the random effects (including random coefficiets).\n\nHat tip to Patrick Ward for the [inspiration](https://optimumsportsperformance.com/blog/mixed-models-in-sport-science-frequentist-bayesian). Go check out his blog for more Mixed Model content!\n\nHo T, Carl S (2024). nflreadr: Download 'nflverse' Data. R package version 1.4.1.05, https://github.com/nflverse/nflreadr, https://nflreadr.nflverse.com.\n\nKuhn et al., (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org\n\nPatrick. (2022, July 11). Mixed Models in Sport Science – Frequentist & Bayesian \\| Patrick Ward, PhD. Retrieved May 3, 2025, from Optimumsportsperformance.com website: https://optimumsportsperformance.com/blog/mixed-models-in-sport-science-frequentist-bayesian/\n",
    "supporting": [
      "wr-mixed-model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}