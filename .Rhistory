glm_model <- linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# Define the parameter grid
param_grid <- grid_regular(penalty(), mixture(), levels = 10)  # Adjust levels as needed
# Set up cross-validation folds
cv_folds <- vfold_cv(train_data, v = 5)
# Create a workflow
workflow <- workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_model)
# Tune the model
tuned_results <- tune_grid(
workflow,
resamples = cv_folds,
grid = param_grid
)
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
# Show the tuning results
tuned_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
par(mfrow = c(2, 2))
plot(final_glm_fit)
# Select the best hyperparameters based on RMSE
best_glm <- select_best(glm_tune_results, metric = 'rmse')
# Finalize the workflow with the best hyperparameters
final_glm_workflow <- finalize_workflow(glm_wflow, best_glm)
best_glm
# Fit the finalized model on the entire training data
final_glm_fit <- fit(final_glm_workflow, data = train_data)
# Make predictions on the test set
glm_predictions <- augment(final_glm_fit, new_data = test_data)
# Evaluate the model's performance (RMSE)
glm_metrics <- glm_predictions %>%
metrics(truth = fantasy_points_target, estimate = .pred)
# Print the evaluation metrics
print(glm_metrics)
glm_predictions %>%
mutate(resid = fantasy_points_target - .pred) %>%
ggplot(aes(x = resid)) +
geom_histogram()
par(mfrow = c(2, 2))
plot(final_glm_fit)
final_glm_fit$fit
final_glm_fit$fit$fit$fit
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = 0,    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(nflreadr)
library(tidymodels)
library(nflfastR)
stats24 = load_player_stats(2024)
stats = load_player_stats(seasons = seq(2006,2023))
pbp23 = load_pbp(2023)
dc = load_depth_charts(season = seq(2016,most_recent_season())) %>% filter(position == 'WR',formation == 'Offense') %>%
select(season,recent_team = club_code,week,season_type = game_type,player_id = gsis_id,depth_team)
# Filter for wide receiver plays
wr_data <- stats %>%
filter(position == "WR") %>%
select(player_id,player_name,position,recent_team,season,week,season_type,
receptions:fantasy_points_ppr) %>%
# Receiving FP
mutate(rec_fp = (receiving_yards * 0.1) + (receiving_tds * 6) + (receptions * 0.5)) %>%
# Add depth chart status since we don't have participation data
left_join(y = dc,by = c('player_id','week','season','season_type',"recent_team")) %>%
# Only first 3 are counted so some players are NA'd if they're below 3rd on DC
replace_na(list(depth_team = '4')) %>%
mutate(depth_team = as.numeric(depth_team))
read.csv('https://github.com/ffverse/ffopportunity/releases/download/latest-data/ep_weekly_2024.csv') -> ff_2024
ff_2024 %>% filter(position == 'WR') %>% select(game_id,player_id,season,rec_fantasy_points,rec_fantasy_points_exp) %>%
metrics(truth = rec_fantasy_points,estimate = rec_fantasy_points_exp)
library(naniar)
gg_miss_case(wr_data)
gg_miss_var(wr_data)
wr_data %>%
select(racr,air_yards_share,wopr,target_share,receiving_epa) -> df_miss
df_miss %>%
gg_miss_upset()
# What if we take out racr?
# Create Time Weighting
weighted_avg = function(metric_vector){
# Take in sliding window of vector of chosen metric
n = length(metric_vector)
# Create Weights for each value based on recency
weights = seq(1,n)
# Calculated weighted average
w_avg = sum(metric_vector * weights) / sum(weights)
return(w_avg)
}
library(TTR)
library(slider)
wr_data %>%
# Should remove most missingness
filter(!is.na(racr)) %>%
group_by(player_id,season) %>%
arrange(week) %>%
# Weighted Avg (moving)
# Take lag so we are not leaking any data
mutate(across(receptions:depth_team, ~ lag(slide_dbl(.x,.f = weighted_avg,.before = Inf,.complete = TRUE)),.names = "wt_{col}")) %>%
ungroup() %>%
# Convert negative fantasy points to 0
mutate(fantasy_points_target = ifelse(fantasy_points < 0,0,fantasy_points),
log_fantasy_points = log(fantasy_points_target + 1)) %>%
# Need data, don't use week 1 for now
filter(week > 1) -> ma_wr
# Calculate the correlation matrix
cor_matrix <- ma_wr %>%
select(starts_with("wt_"), fantasy_points_target) %>%
cor(use = "complete.obs")
# Reshape the correlation matrix for ggplot
cor_data <- reshape2::melt(cor_matrix)
ggplot(data = cor_data, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), name="Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
labs(title = "Correlation Matrix of Dataset", x = "", y = "")
ma_wr %>%
ggplot(aes(x = fantasy_points_target)) +
geom_histogram()
ma_wr %>%
ggplot(aes(x = log_fantasy_points)) +
geom_histogram()
library(tidymodels)
# Split
set.seed(222)
# Put 3/4 of the data into the training set
data_split <- ma_wr %>%
# Don't use first week
filter(week >1)%>%
# Filter on relevant columns
select(starts_with('wt_'),fantasy_points_target,player_id,season,week,recent_team,
fantasy_points,fantasy_points_ppr) %>%
# make split
initial_split( prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
wt_recipe = train_data %>%
recipe(fantasy_points_target ~ .,) %>%
update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr),new_role = 'ID') %>%
# Generally not recommended to throw out all data, but for brevity, let's remove NAs
step_naomit(all_numeric_predictors()) %>%
# Remove zero variance predictors (ie. variables that contribute nothing to prediction)
step_center(all_numeric_predictors())
#
summary(wt_recipe)
test_data %>%
filter(!is.na(wt_receptions)) -> test_data
sum(colSums(is.na(test_data)))
exp_recipe = train_data  %>%
recipe(fantasy_points_target ~ .,) %>%
update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr),new_role = 'ID') %>%
# Generally not recommended to throw out all data, but for brevity, let's remove NAs
step_naomit(all_numeric_predictors()) %>%
# Remove zero variance predictors (ie. variables that contribute nothing to prediction)
step_zv(all_predictors()) %>%
step_center(all_numeric_predictors())
#summary(exp_recipe)
# Specify a penalized GLM model with tuning parameters
glm_model <- linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# Define the parameter grid
param_grid <- grid_regular(penalty(), mixture(), levels = 10)  # Adjust levels as needed
# Set up cross-validation folds
cv_folds <- vfold_cv(train_data, v = 5)
# Create a workflow
workflow <- workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_model)
# Tune the model
tuned_results <- tune_grid(
workflow,
resamples = cv_folds,
grid = param_grid
)
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = 0,    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = 1,    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
# Select the best hyperparameters based on RMSE
best_glm <- select_best(glm_tune_results, metric = 'rmse')
# Finalize the workflow with the best hyperparameters
final_glm_workflow <- finalize_workflow(glm_wflow, best_glm)
best_glm
best_glm$.config
best_glm
final_glm_workflow$fit$actions$model$formula
final_glm_workflow$fit$actions$model$spec
final_glm_workflow$fit$fit
best_glm
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
?hclust
library('nflfastR')
library("tidyverse")
library('pROC')
library('tidymodels')
data <- load_pbp(2019)
View(data)
pass_plays = data %>% filter(play_type == 'pass')
View(pass_plays)
pass_plays$pass_oe
pass_plays$play_type_nfl
pass_plays$play_type_nfl %>% unique()
# Check the unique play results
pass_plays$play_type_nfl %>% unique()
# Check the unique play results
pass_plays %>%
group_by(play_type_nfl) %>%
tally()
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa,series_result) %>%
pivot_longer(cols = c(complete_pass:interception))
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception))
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(expected_points_added))
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa))
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T))
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.))
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.)) %>%
ungroup() %>%
mutate(ev = EPA * .prop)
# Check the unique play results
data %>%
group_by(play_type_nfl) %>%
tally()
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.))
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.))
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.)) %>%
ungroup() %>%
mutate(plot_color = ifelse(EPA < 0, "red", "blue")) %>%
ggplot(aes(x = play_result, y = EPA,fill = plot_color)) +
geom_col() +
labs(title = "", x = "Result", y = "EPA") +
theme_minimal() +
guides(fill="none") +
scale_fill_brewer(palette = "Reds")
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.)) %>%
ungroup() %>%
mutate(plot_color = ifelse(EPA < 0, "red", "blue")) %>%
ggplot(aes(x = play_result, y = EPA,fill = plot_color)) +
geom_col() +
labs(title = "Average Expected Points Added", x = "Result", y = "EPA") +
theme_minimal() +
guides(fill="none") +
scale_fill_brewer(palette = "Reds")
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.)) %>%
ungroup() %>%
mutate(plot_color = ifelse(EPA < 0, "red", "blue")) %>%
ggplot(aes(x = play_result, y = EPA,fill = plot_color)) +
geom_col() +
geom_label(aes(text = round(EPA)))
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.)) %>%
ungroup() %>%
mutate(plot_color = ifelse(EPA < 0, "red", "blue")) %>%
ggplot(aes(x = play_result, y = EPA,fill = plot_color)) +
geom_col() +
geom_label(aes(text = round(EPA))) +
labs(title = "Average Expected Points Added", x = "Result", y = "EPA") +
theme_minimal() +
guides(fill="none") +
scale_fill_brewer(palette = "Reds")
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.)) %>%
ungroup() %>%
mutate(plot_color = ifelse(EPA < 0, "red", "blue")) %>%
ggplot(aes(x = play_result, y = EPA,fill = plot_color)) +
geom_col() +
geom_text(aes(text = round(EPA))) +
labs(title = "Average Expected Points Added", x = "Result", y = "EPA") +
theme_minimal() +
guides(fill="none") +
scale_fill_brewer(palette = "Reds")
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.)) %>%
ungroup() %>%
mutate(plot_color = ifelse(EPA < 0, "red", "blue")) %>%
ggplot(aes(x = play_result, y = EPA,fill = plot_color)) +
geom_col() +
geom_text(aes(label = round(EPA))) +
labs(title = "Average Expected Points Added", x = "Result", y = "EPA") +
theme_minimal() +
guides(fill="none") +
scale_fill_brewer(palette = "Reds")
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.)) %>%
ungroup() %>%
mutate(plot_color = ifelse(EPA < 0, "red", "blue")) %>%
ggplot(aes(x = play_result, y = EPA,fill = plot_color)) +
geom_col() +
geom_text(aes(label = round(EPA,2))) +
labs(title = "Average Expected Points Added", x = "Result", y = "EPA") +
theme_minimal() +
guides(fill="none") +
scale_fill_brewer(palette = "Reds")
nflreadr::load_pbp(2019) -> readr_pbp
data$pass
pass_plays %>%
select(complete_pass,incomplete_pass,sack,interception,epa) %>%
pivot_longer(cols = c(complete_pass:interception),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.),ev = EPA *.prop)
data %>%
select(pass, rush,epa) %>%
pivot_longer(cols = c(pass,rush),names_to = "play_result") %>%
filter(value == 1) %>%
group_by(play_result) %>%
summarise(EPA = mean(epa,na.rm = T),.prop = n()/ nrow(.),ev = EPA *.prop)
ng_pass = nflreadr::load_nextgen_stats(2019, 'passing')
View(ng_pass)
View(ng_pass)
library(nflreadr)
load_pfr_advstats(2019,'def','week') -> pfr_def
View(pfr_def)
load_pfr_advstats(2019,'pass','week') -> pfr_def
View(pfr_def)
load_pfr_advstats(2019,'pass','week') -> pfr_pass
load_pfr_advstats(2019,'def','week') -> pfr_def
View(ng_pass)
View(pfr_pass)
load_pfr_advstats(2015,'def','week') -> pfr_def
