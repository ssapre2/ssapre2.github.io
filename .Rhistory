#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
# Show the tuning results
tuned_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
par(mfrow = c(2, 2))
plot(final_glm_fit)
# Select the best hyperparameters based on RMSE
best_glm <- select_best(glm_tune_results, metric = 'rmse')
# Finalize the workflow with the best hyperparameters
final_glm_workflow <- finalize_workflow(glm_wflow, best_glm)
best_glm
# Fit the finalized model on the entire training data
final_glm_fit <- fit(final_glm_workflow, data = train_data)
# Make predictions on the test set
glm_predictions <- augment(final_glm_fit, new_data = test_data)
# Evaluate the model's performance (RMSE)
glm_metrics <- glm_predictions %>%
metrics(truth = fantasy_points_target, estimate = .pred)
# Print the evaluation metrics
print(glm_metrics)
glm_predictions %>%
mutate(resid = fantasy_points_target - .pred) %>%
ggplot(aes(x = resid)) +
geom_histogram()
par(mfrow = c(2, 2))
plot(final_glm_fit)
final_glm_fit$fit
final_glm_fit$fit$fit$fit
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = 0,    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(nflreadr)
library(tidymodels)
library(nflfastR)
stats24 = load_player_stats(2024)
stats = load_player_stats(seasons = seq(2006,2023))
pbp23 = load_pbp(2023)
dc = load_depth_charts(season = seq(2016,most_recent_season())) %>% filter(position == 'WR',formation == 'Offense') %>%
select(season,recent_team = club_code,week,season_type = game_type,player_id = gsis_id,depth_team)
# Filter for wide receiver plays
wr_data <- stats %>%
filter(position == "WR") %>%
select(player_id,player_name,position,recent_team,season,week,season_type,
receptions:fantasy_points_ppr) %>%
# Receiving FP
mutate(rec_fp = (receiving_yards * 0.1) + (receiving_tds * 6) + (receptions * 0.5)) %>%
# Add depth chart status since we don't have participation data
left_join(y = dc,by = c('player_id','week','season','season_type',"recent_team")) %>%
# Only first 3 are counted so some players are NA'd if they're below 3rd on DC
replace_na(list(depth_team = '4')) %>%
mutate(depth_team = as.numeric(depth_team))
read.csv('https://github.com/ffverse/ffopportunity/releases/download/latest-data/ep_weekly_2024.csv') -> ff_2024
ff_2024 %>% filter(position == 'WR') %>% select(game_id,player_id,season,rec_fantasy_points,rec_fantasy_points_exp) %>%
metrics(truth = rec_fantasy_points,estimate = rec_fantasy_points_exp)
library(naniar)
gg_miss_case(wr_data)
gg_miss_var(wr_data)
wr_data %>%
select(racr,air_yards_share,wopr,target_share,receiving_epa) -> df_miss
df_miss %>%
gg_miss_upset()
# What if we take out racr?
# Create Time Weighting
weighted_avg = function(metric_vector){
# Take in sliding window of vector of chosen metric
n = length(metric_vector)
# Create Weights for each value based on recency
weights = seq(1,n)
# Calculated weighted average
w_avg = sum(metric_vector * weights) / sum(weights)
return(w_avg)
}
library(TTR)
library(slider)
wr_data %>%
# Should remove most missingness
filter(!is.na(racr)) %>%
group_by(player_id,season) %>%
arrange(week) %>%
# Weighted Avg (moving)
# Take lag so we are not leaking any data
mutate(across(receptions:depth_team, ~ lag(slide_dbl(.x,.f = weighted_avg,.before = Inf,.complete = TRUE)),.names = "wt_{col}")) %>%
ungroup() %>%
# Convert negative fantasy points to 0
mutate(fantasy_points_target = ifelse(fantasy_points < 0,0,fantasy_points),
log_fantasy_points = log(fantasy_points_target + 1)) %>%
# Need data, don't use week 1 for now
filter(week > 1) -> ma_wr
# Calculate the correlation matrix
cor_matrix <- ma_wr %>%
select(starts_with("wt_"), fantasy_points_target) %>%
cor(use = "complete.obs")
# Reshape the correlation matrix for ggplot
cor_data <- reshape2::melt(cor_matrix)
ggplot(data = cor_data, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), name="Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
labs(title = "Correlation Matrix of Dataset", x = "", y = "")
ma_wr %>%
ggplot(aes(x = fantasy_points_target)) +
geom_histogram()
ma_wr %>%
ggplot(aes(x = log_fantasy_points)) +
geom_histogram()
library(tidymodels)
# Split
set.seed(222)
# Put 3/4 of the data into the training set
data_split <- ma_wr %>%
# Don't use first week
filter(week >1)%>%
# Filter on relevant columns
select(starts_with('wt_'),fantasy_points_target,player_id,season,week,recent_team,
fantasy_points,fantasy_points_ppr) %>%
# make split
initial_split( prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
wt_recipe = train_data %>%
recipe(fantasy_points_target ~ .,) %>%
update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr),new_role = 'ID') %>%
# Generally not recommended to throw out all data, but for brevity, let's remove NAs
step_naomit(all_numeric_predictors()) %>%
# Remove zero variance predictors (ie. variables that contribute nothing to prediction)
step_center(all_numeric_predictors())
#
summary(wt_recipe)
test_data %>%
filter(!is.na(wt_receptions)) -> test_data
sum(colSums(is.na(test_data)))
exp_recipe = train_data  %>%
recipe(fantasy_points_target ~ .,) %>%
update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr),new_role = 'ID') %>%
# Generally not recommended to throw out all data, but for brevity, let's remove NAs
step_naomit(all_numeric_predictors()) %>%
# Remove zero variance predictors (ie. variables that contribute nothing to prediction)
step_zv(all_predictors()) %>%
step_center(all_numeric_predictors())
#summary(exp_recipe)
# Specify a penalized GLM model with tuning parameters
glm_model <- linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# Define the parameter grid
param_grid <- grid_regular(penalty(), mixture(), levels = 10)  # Adjust levels as needed
# Set up cross-validation folds
cv_folds <- vfold_cv(train_data, v = 5)
# Create a workflow
workflow <- workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_model)
# Tune the model
tuned_results <- tune_grid(
workflow,
resamples = cv_folds,
grid = param_grid
)
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = 0,    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = 1,    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
# Select the best hyperparameters based on RMSE
best_glm <- select_best(glm_tune_results, metric = 'rmse')
# Finalize the workflow with the best hyperparameters
final_glm_workflow <- finalize_workflow(glm_wflow, best_glm)
best_glm
best_glm$.config
best_glm
final_glm_workflow$fit$actions$model$formula
final_glm_workflow$fit$actions$model$spec
final_glm_workflow$fit$fit
best_glm
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
?hclust
#| code-fold: true
library('nflreadr')
library("tidyverse")
library('pROC')
library('tidymodels')
# Scrape schedule Results
load_schedules(seasons = seq(2011,2024)) -> nfl_game_results
nfl_game_results %>%
# Remove the upcoming season
pivot_longer(cols = c(away_team,home_team),
names_to = "home_away",
values_to = "team") %>%
mutate(team_score = ifelse(home_away == "home_team",yes = home_score,no = away_score),
opp_score = ifelse(home_away == "home_team", away_score,home_score)) %>%  # sort for cumulative avg
arrange(season,week) %>%
select(season,game_id,team,team_score,opp_score,week) -> team_games
team_games %>%
arrange(week) %>%
group_by(season,team) %>%
# For each team's season calculate the cumulative scores for after each week
mutate(cumul_score_mean = cummean(team_score),
cumul_score_opp = cummean(opp_score),
cumul_wins = cumsum(team_score > opp_score),
cumul_losses = cumsum(team_score < opp_score),
cumul_ties = cumsum(team_score == opp_score),
cumul_win_pct = cumul_wins / (cumul_wins + cumul_losses),
# Create the lag variable
cumul_win_pct_lag_1 = lag(cumul_win_pct,1),
cumul_score_lag_1 = lag(cumul_score_mean,1),
cumul_opp_lag_1 = lag(cumul_score_opp,1)
) %>%
# Non-lag variables leak info
select(week,game_id,contains('lag_1')) %>%
ungroup() -> cumul_avgs
# Calculate average win pct.
team_games %>%
group_by(season,team) %>%
summarise(wins = sum(team_score > opp_score),
losses = sum(team_score < opp_score),
ties = sum(team_score == opp_score))%>%
ungroup() %>%
arrange(season) %>%
group_by(team) %>%
mutate(win_pct = wins / (wins + losses),
lag1_win_pct = lag(win_pct,1)) %>%
ungroup() -> team_win_pct
# Load depth charts and injury reports
dc = load_depth_charts(seq(2011,most_recent_season()))
injuries = load_injuries(seq(2011,most_recent_season()))
injuries %>%
filter(report_status == "Out") -> out_inj
dc %>%
filter(depth_team == 1) -> starters
# Determine roster position of injured players
starters %>%
select(-c(last_name,first_name,position,full_name)) %>%
inner_join(out_inj, by = c('season','club_code' = 'team','gsis_id','game_type','week')) -> injured_starters
# Number of injuries by position
injured_starters %>%
group_by(season,club_code,week,position) %>%
summarise(starters_injured = n()) %>%
ungroup() %>%
pivot_wider(names_from = position, names_prefix = "injured_",values_from = starters_injured) -> injuries_position
nfl_game_results %>%
inner_join(cumul_avgs, by = c('game_id','season','week','home_team' = 'team')) %>%
inner_join(cumul_avgs, by = c('game_id','season','week','away_team' = 'team'),suffix = c('_home','_away'))-> w_avgs
# Check for stragglers
nfl_game_results %>%
anti_join(cumul_avgs, by = c('game_id','season','home_team' = 'team','week')) -> unplayed_games
# Indicate whether home team won
w_avgs %>%
mutate(home_win = as.numeric(result > 0)) -> matchups
matchups %>%
left_join(injuries_position,by = c('season','home_team'='club_code','week')) %>%
left_join(injuries_position,by = c('season','away_team'='club_code','week'),suffix = c('_home','_away')) %>%
mutate(across(starts_with('injured_'), ~replace_na(.x, 0))) -> matchup_full
# Remove unneeded columns
matchup_full %>%
select(-c(home_score,away_score,overtime,home_team,away_team,away_qb_name,home_qb_name,referee,stadium,home_coach,away_coach,ftn,espn,old_game_id,gsis,nfl_detail_id,pfr,pff,result)) -> matchup_ready
# Remove columns
matchup_ready = matchup_ready%>%
# Transform outcome into factor variable
select(where(is.numeric),game_id) %>%
mutate(home_win = as.factor(home_win))
library('tidymodels')
# Split Data
set.seed(123)
matchups24 = matchup_ready %>% filter(season == 2024)
splits = matchup_ready %>%
filter(season != 2024) %>%
initial_split(prop = 0.7)
train_data <- training(splits)
test_data  <- testing(splits)
rec_impute = recipe(formula = home_win ~ .,
data = train_data) %>%
#create ID role (do not remove) game ID. We'll use this to match predictions to specific games
update_role(game_id, new_role = "ID")%>%
step_zv(all_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
#for each numeric variable/feature, replace any NA's with the median value of that variable/feature
step_impute_median(all_numeric_predictors())
imp_models <- rec_impute %>%
check_missing(all_numeric_predictors()) %>%
prep(training = train_data)
# Check the predictors that were filtered out
nzv_step <- imp_models$steps[[2]]  # Access the step_nzv object
removed_predictors <- nzv_step$removals
# Display removed predictors
removed_predictors
# Check if imputation worked
imp_models %>%
bake(train_data) %>%
is.na() %>%
colSums()
tidy(imp_models,number = 1)
library('glmnet')
# Penalized Linear Regression
glm_spec <- logistic_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet")
# Setup workflow
glm_wflow <-
workflow() %>%
add_recipe(rec_impute) %>%
add_model(glm_spec)
# Create cross validation splits
folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = folds,
metrics = 'brier_class'
grid = 10   # Number of tuning combinations to evaluate
library('glmnet')
# Penalized Linear Regression
glm_spec <- logistic_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet")
# Setup workflow
glm_wflow <-
workflow() %>%
add_recipe(rec_impute) %>%
add_model(glm_spec)
# Create cross validation splits
folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = folds,
metrics = 'brier_class',
grid = 10   # Number of tuning combinations to evaluate
)
yardstick::metric_set()
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = folds,
metrics = brier_class(),
grid = 10   # Number of tuning combinations to evaluate
)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = folds,
metrics = metric_set(brier_class),
grid = 10   # Number of tuning combinations to evaluate
)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "brier_class") %>%
arrange(mean) %>%
slice(1)
matchup_ready %>% head()
matchup_ready %>% head(-1)
View(matchups24)
matchup_ready[400:405]
matchup_ready[400:405,]
# install and load packages
#install.packages("nflreadr")
library('nflreadr')
library("tidyverse")
library('pROC')
library('tidymodels')
# Scrape schedule Results
load_schedules(seasons = seq(2011,2024)) -> nfl_game_results
head(nfl_game_results)
View(nfl_game_results)
load_nextgen_stats(
seasons = TRUE,
stat_type = c("defense"),
file_type = getOption("nflreadr.prefer", default = "rds")
)
# Scrape schedule Results
load_schedules(seasons = seq(2011,2024)) -> nfl_game_results
# install and load packages
#install.packages("nflreadr")
library('nflreadr')
library("tidyverse")
library('pROC')
library('tidymodels')
# Scrape schedule Results
load_schedules(seasons = seq(2011,2024)) -> nfl_game_results
View(nfl_game_results)
stats = load_player_stats(seasons = seq(2006,2023),stat_type = 'defense')
View(stats)
stats |>  group_by(team,week,season) |> summarise(total_tackles = sum(def_tackles),forced_fumbles = sum(def_fumbles_forced))
stats |>  group_by(team,week,season) |> summarise(total_tackles = sum(def_tackles),forced_fumbles = sum(def_fumbles_forced)) -> weekly_fumbles_by_team
pbp = load_pbp(2021)
View(pbp)
