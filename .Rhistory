add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = 1,    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
# Select the best hyperparameters based on RMSE
best_glm <- select_best(glm_tune_results, metric = 'rmse')
# Finalize the workflow with the best hyperparameters
final_glm_workflow <- finalize_workflow(glm_wflow, best_glm)
best_glm
best_glm$.config
best_glm
final_glm_workflow$fit$actions$model$formula
final_glm_workflow$fit$actions$model$spec
final_glm_workflow$fit$fit
best_glm
library(glmnet)
#library(lightgbm)
#library(bonsai)
glm_spec <- linear_reg(
penalty = tune(),     # Lambda (regularization strength)
mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
?hclust
#| code-fold: true
library('nflreadr')
library("tidyverse")
library('pROC')
library('tidymodels')
# Scrape schedule Results
load_schedules(seasons = seq(2011,2024)) -> nfl_game_results
nfl_game_results %>%
# Remove the upcoming season
pivot_longer(cols = c(away_team,home_team),
names_to = "home_away",
values_to = "team") %>%
mutate(team_score = ifelse(home_away == "home_team",yes = home_score,no = away_score),
opp_score = ifelse(home_away == "home_team", away_score,home_score)) %>%  # sort for cumulative avg
arrange(season,week) %>%
select(season,game_id,team,team_score,opp_score,week) -> team_games
team_games %>%
arrange(week) %>%
group_by(season,team) %>%
# For each team's season calculate the cumulative scores for after each week
mutate(cumul_score_mean = cummean(team_score),
cumul_score_opp = cummean(opp_score),
cumul_wins = cumsum(team_score > opp_score),
cumul_losses = cumsum(team_score < opp_score),
cumul_ties = cumsum(team_score == opp_score),
cumul_win_pct = cumul_wins / (cumul_wins + cumul_losses),
# Create the lag variable
cumul_win_pct_lag_1 = lag(cumul_win_pct,1),
cumul_score_lag_1 = lag(cumul_score_mean,1),
cumul_opp_lag_1 = lag(cumul_score_opp,1)
) %>%
# Non-lag variables leak info
select(week,game_id,contains('lag_1')) %>%
ungroup() -> cumul_avgs
# Calculate average win pct.
team_games %>%
group_by(season,team) %>%
summarise(wins = sum(team_score > opp_score),
losses = sum(team_score < opp_score),
ties = sum(team_score == opp_score))%>%
ungroup() %>%
arrange(season) %>%
group_by(team) %>%
mutate(win_pct = wins / (wins + losses),
lag1_win_pct = lag(win_pct,1)) %>%
ungroup() -> team_win_pct
# Load depth charts and injury reports
dc = load_depth_charts(seq(2011,most_recent_season()))
injuries = load_injuries(seq(2011,most_recent_season()))
injuries %>%
filter(report_status == "Out") -> out_inj
dc %>%
filter(depth_team == 1) -> starters
# Determine roster position of injured players
starters %>%
select(-c(last_name,first_name,position,full_name)) %>%
inner_join(out_inj, by = c('season','club_code' = 'team','gsis_id','game_type','week')) -> injured_starters
# Number of injuries by position
injured_starters %>%
group_by(season,club_code,week,position) %>%
summarise(starters_injured = n()) %>%
ungroup() %>%
pivot_wider(names_from = position, names_prefix = "injured_",values_from = starters_injured) -> injuries_position
nfl_game_results %>%
inner_join(cumul_avgs, by = c('game_id','season','week','home_team' = 'team')) %>%
inner_join(cumul_avgs, by = c('game_id','season','week','away_team' = 'team'),suffix = c('_home','_away'))-> w_avgs
# Check for stragglers
nfl_game_results %>%
anti_join(cumul_avgs, by = c('game_id','season','home_team' = 'team','week')) -> unplayed_games
# Indicate whether home team won
w_avgs %>%
mutate(home_win = as.numeric(result > 0)) -> matchups
matchups %>%
left_join(injuries_position,by = c('season','home_team'='club_code','week')) %>%
left_join(injuries_position,by = c('season','away_team'='club_code','week'),suffix = c('_home','_away')) %>%
mutate(across(starts_with('injured_'), ~replace_na(.x, 0))) -> matchup_full
# Remove unneeded columns
matchup_full %>%
select(-c(home_score,away_score,overtime,home_team,away_team,away_qb_name,home_qb_name,referee,stadium,home_coach,away_coach,ftn,espn,old_game_id,gsis,nfl_detail_id,pfr,pff,result)) -> matchup_ready
# Remove columns
matchup_ready = matchup_ready%>%
# Transform outcome into factor variable
select(where(is.numeric),game_id) %>%
mutate(home_win = as.factor(home_win))
library('tidymodels')
# Split Data
set.seed(123)
matchups24 = matchup_ready %>% filter(season == 2024)
splits = matchup_ready %>%
filter(season != 2024) %>%
initial_split(prop = 0.7)
train_data <- training(splits)
test_data  <- testing(splits)
rec_impute = recipe(formula = home_win ~ .,
data = train_data) %>%
#create ID role (do not remove) game ID. We'll use this to match predictions to specific games
update_role(game_id, new_role = "ID")%>%
step_zv(all_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
#for each numeric variable/feature, replace any NA's with the median value of that variable/feature
step_impute_median(all_numeric_predictors())
imp_models <- rec_impute %>%
check_missing(all_numeric_predictors()) %>%
prep(training = train_data)
# Check the predictors that were filtered out
nzv_step <- imp_models$steps[[2]]  # Access the step_nzv object
removed_predictors <- nzv_step$removals
# Display removed predictors
removed_predictors
# Check if imputation worked
imp_models %>%
bake(train_data) %>%
is.na() %>%
colSums()
tidy(imp_models,number = 1)
library('glmnet')
# Penalized Linear Regression
glm_spec <- logistic_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet")
# Setup workflow
glm_wflow <-
workflow() %>%
add_recipe(rec_impute) %>%
add_model(glm_spec)
# Create cross validation splits
folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = folds,
metrics = 'brier_class'
grid = 10   # Number of tuning combinations to evaluate
library('glmnet')
# Penalized Linear Regression
glm_spec <- logistic_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet")
# Setup workflow
glm_wflow <-
workflow() %>%
add_recipe(rec_impute) %>%
add_model(glm_spec)
# Create cross validation splits
folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = folds,
metrics = 'brier_class',
grid = 10   # Number of tuning combinations to evaluate
)
yardstick::metric_set()
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = folds,
metrics = brier_class(),
grid = 10   # Number of tuning combinations to evaluate
)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = folds,
metrics = metric_set(brier_class),
grid = 10   # Number of tuning combinations to evaluate
)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "brier_class") %>%
arrange(mean) %>%
slice(1)
matchup_ready %>% head()
matchup_ready %>% head(-1)
View(matchups24)
matchup_ready[400:405]
matchup_ready[400:405,]
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(nflreadr)
library(tidymodels)
library(nflfastR)
library(naniar)
library(TTR)
stats = load_player_stats(seasons = seq(2006,2023))
dc = load_depth_charts(season = seq(2016,most_recent_season())) %>% filter(position == 'WR',formation == 'Offense') %>%
select(season,recent_team = club_code,week,season_type = game_type,player_id = gsis_id,depth_team)
# Filter for wide receiver plays
wr_data <- stats %>%
filter(position == "WR") %>%
# Select identifying info + receiver specfic metrics
select(player_id,player_name,position,recent_team,season,week,season_type,
receptions:fantasy_points_ppr) %>%
# Add depth chart status since we don't have participation data
left_join(y = dc,by = c('player_id','week','season','season_type',"recent_team")) %>%
# Only first 3 are counted so some players are NA'd if they're below 3rd on DC
replace_na(list(depth_team = '4')) %>%
mutate(depth_team = as.numeric(depth_team))
# Explore variables patterns of missingness
gg_miss_var(wr_data)
wr_data %>%
select(racr,air_yards_share,wopr,target_share,receiving_epa) -> df_miss
df_miss %>%
gg_miss_upset()
# What if we take out racr?
# Create Time Weighting
weighted_avg = function(metric_vector){
# Take in sliding window of vector of chosen metric
n = length(metric_vector)
# Create Weights for each value based on recency
weights = seq(1,n)
# Calculated weighted average
w_avg = sum(metric_vector * weights) / sum(weights)
return(w_avg)
}
library(slider)
wr_data %>%
# Should remove most missingness
filter(!is.na(racr)) %>%
group_by(player_id,season) %>%
arrange(week) %>%
# Weighted Avg (moving)
# Take lag so we are not leaking data
mutate(across(receptions:depth_team, ~ lag(slide_dbl(.x,.f = weighted_avg,.before = Inf,.complete = TRUE)),.names = "wt_{col}")) %>%
ungroup() %>%
# Convert negative fantasy points to 0
mutate(fantasy_points_target = ifelse(fantasy_points < 0,0,fantasy_points),
# Take log (more on this later)
log_fantasy_points = log(fantasy_points_target + 1)) -> ma_wr
ma_wr %>%
filter(season == 2023, player_id == '00-0036261') %>%
select(player_name,week,targets,wt_targets, receptions,wt_receptions, receiving_yards,wt_receiving_yards)
# Calculate the correlation matrix
cor_matrix <- ma_wr %>%
select(starts_with("wt_"), fantasy_points_target) %>%
cor(use = "complete.obs")
# Reshape the correlation matrix for ggplot
cor_data <- reshape2::melt(cor_matrix)
ggplot(data = cor_data, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), name="Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
labs(title = "Correlation Matrix of Dataset", x = "", y = "")
ma_wr %>%
ggplot(aes(x = fantasy_points_target)) +
geom_histogram()
ma_wr %>%
ggplot(aes(x = log_fantasy_points)) +
geom_histogram()
library(tidymodels)
# Split
set.seed(222)
# Put 3/4 of the data into the training set
data_split <- ma_wr %>%
# Filter on relevant columns
select(starts_with('wt_'),fantasy_points_target,player_id,season,week,recent_team,
fantasy_points,fantasy_points_ppr,log_fantasy_points) %>%
# make split
initial_split(prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
test_data %>%
filter(!is.na(wt_receptions)) -> test_data
sum(colSums(is.na(test_data)))
exp_recipe = train_data  %>%
recipe(log_fantasy_points ~ .,) %>%
update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr,fantasy_points_target),new_role = 'ID') %>%
# Generally not recommended to throw out all data, but for brevity, let's remove NAs
step_impute_median(all_numeric_predictors()) %>%
# Remove zero variance predictors (ie. variables that contribute nothing to prediction)
step_zv(all_predictors()) %>%
step_center(all_numeric_predictors())
#summary(exp_recipe)
library(glmnet)
glm_spec <- linear_reg(
penalty = tune(),
mixture = tune(),    # Mixutre (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
) %>%
set_engine("glmnet")
# Setup workflow
glm_wflow <-
workflow() %>%
add_recipe(exp_recipe) %>%
add_model(glm_spec)
# Create cross validation splits
wr_folds <- vfold_cv(train_data, v = 5)
# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
glm_wflow,
resamples = wr_folds,
grid = 10   # Number of tuning combinations to evaluate
)
# Display tuning results
glm_tune_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
arrange(mean)
# Select the best hyperparameters based on RMSE
best_glm <- select_best(glm_tune_results, metric = 'rmse')
# Finalize the workflow with the best hyperparameters
final_glm_workflow <- finalize_workflow(glm_wflow, best_glm)
best_glm
# Fit the finalized model on the entire training data
final_glm_fit <- fit(final_glm_workflow, data = train_data)
# Make predictions on the test set and tidy
glm_predictions <- augment(final_glm_fit, new_data = test_data) %>%
mutate(.pred_fp = exp(.pred) + 1,
.resid = fantasy_points - .pred_fp)
# Evaluate the model's performance (RMSE)
glm_metrics <- glm_predictions %>%
metrics(truth = fantasy_points, estimate = .pred_fp)
# Print the evaluation metrics
print(glm_metrics)
# Distribution of predictors
glm_predictions  %>%
ggplot(aes(x = .resid)) +
geom_histogram()
# Residuals vs fitted
ggplot(glm_predictions,aes(x = .pred_fp, y=.resid)) +
geom_point() +
geom_smooth(se = F) +
geom_hline(yintercept = 0,linetype = "dashed",color = "red")+
labs(title = "Residuals vs. Fitted", y= "Residuals",x = "Fitted")
ggplot(glm_predictions,aes(sample = .resid)) +
geom_qq() +
geom_qq_line() +
labs(title = "Q-Q Plot of Residuals") +
theme_minimal()
final_glm_fit %>%
vip::vi() %>%
mutate(Importance = abs(Importance),
Variable = fct_reorder(Variable,Importance)) %>%
ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
geom_col() +
scale_x_continuous(expand = c(0,0)) +
labs(x = "Impact")
ggplot(train_data, aes(x = wt_receptions, y = wt_receiving_fumbles_lost)) +
geom_smooth() +
theme_minimal() +
labs(y = "Fumbles Lost", x = "Receptions")
ggplot(train_data, aes(x = wt_target_share, y = wt_receiving_fumbles_lost)) +
geom_smooth() +
labs(y = "Fumbles", x= "Target Share")
ma_wr %>%
filter(season == 2023) %>%
group_by(player_id) %>%
mutate(season_catches = sum(receptions)) %>%# %>%
ungroup() %>%
arrange(desc(season_catches)) %>%
top_n(170) %>%
ggplot(aes(x = week, y = fantasy_points)) +
geom_point() +
geom_line() +
facet_wrap(~ player_name)
exp_recipe = train_data  %>%
recipe() %>%
update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr,fantasy_points_target),new_role = 'ID') %>%
# Generally not recommended to throw out all data, but for brevity, let's remove NAs
step_impute_median(all_numeric_predictors()) %>%
# Remove zero variance predictors (ie. variables that contribute nothing to prediction)
step_zv(all_predictors()) %>%
step_center(all_numeric_predictors()) %>%
step_naomit(columns = c(wt_fantasy_points))
exp_recipe = ma_wr %>%
recipe() %>%
update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr,fantasy_points_target),new_role = 'ID') %>%
# Generally not recommended to throw out all data, but for brevity, let's remove NAs
step_impute_median(all_numeric_predictors()) %>%
# Remove zero variance predictors (ie. variables that contribute nothing to prediction)
step_zv(all_predictors()) %>%
step_center(all_numeric_predictors()) %>%
step_naomit(columns = c(wt_fantasy_points))
exp_recipe = train_data %>%
recipe(formula = log_fantasy_points ~ .) %>%
update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr,fantasy_points_target),new_role = 'ID') %>%
# Generally not recommended to throw out all data, but for brevity, let's remove NAs
step_impute_median(all_numeric_predictors()) %>%
# Remove zero variance predictors (ie. variables that contribute nothing to prediction)
step_zv(all_predictors()) %>%
step_center(all_numeric_predictors())
# Setup workflow
glm_wflow <- workflow() %>%
add_recipe(exp_recipe) %>%
add_model(spec = mixed_mod_spec, formula = log_fantasy_points ~ 1 + (1|player_id))
library(lme4)
library(multilevelmod)
mixed_mod_spec <- linear_reg() %>%
set_engine("lmer")
# Setup workflow
glm_wflow <- workflow() %>%
add_recipe(exp_recipe) %>%
add_model(spec = mixed_mod_spec, formula = log_fantasy_points ~ 1 + (1|player_id))
fit(glm_wflow,data = train_data)
exp_recipe = train_data %>%
recipe(formula = log_fantasy_points ~ .) %>%
update_role(c(recent_team,fantasy_points,fantasy_points_ppr,fantasy_points_target),new_role = 'ID') %>%
# Generally not recommended to throw out all data, but for brevity, let's remove NAs
step_impute_median(all_numeric_predictors()) %>%
# Remove zero variance predictors (ie. variables that contribute nothing to prediction)
step_zv(all_predictors()) %>%
step_center(all_numeric_predictors())
library(lme4)
library(multilevelmod)
mixed_mod_spec <- linear_reg() %>%
set_engine("lmer")
# Setup workflow
glm_wflow <- workflow() %>%
add_recipe(exp_recipe) %>%
add_model(spec = mixed_mod_spec, formula = log_fantasy_points ~ 1 + (1|player_id))
fit(glm_wflow,data = train_data)
# Create cross validation splits
#wr_folds <- vfold_cv(train_data, v = 5)
