[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello everyone! My name is Sameer Sapre and I am a Data Scientist with the Seattle Reign of the National Women’s Soccer League (NWSL) and was previously working in the analytics department of the Seattle Mariners. I graduated from Penn State University in 2020 with a B.S. in Data Sciences and completed internships with Mars, Inc and Penn State’s Applied Health and Performance Science department. Outside of work, I’m the President of my local Toastmasters chapter (please join!), a youth soccer coach with Seattle Parks and Rec, and a volunteer with Seattle Theatre Group."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sameer’s Blog",
    "section": "",
    "text": "Predicting NFL Game Outcomes in R (part 2)\n\n\nTuning our regularized regression model\n\n\n\ncode\n\n\nanalysis\n\n\nNFL\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nSameer Sapre\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Wide Receiver Fantasy Points w/ Tidymodels\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\nSameer Sapre\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting NFL Game Outcomes in R (part 1)\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 30, 2024\n\n\nSameer Sapre\n\n\n\n\n\n\n\n\n\n\n\n\nData-Driven Descriptions: Using Machine Learning to Profile 2020 NBA Draft Prospects\n\n\n\n\n\n\nhoops\n\n\n\n\n\n\n\n\n\nNov 27, 2020\n\n\nSameer Sapre\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nfl-game-prediction/index.html",
    "href": "posts/nfl-game-prediction/index.html",
    "title": "Predicting NFL Game Outcomes w/ tidymodels & nflreadR",
    "section": "",
    "text": "Hello everyone! Today I want to share a tutorial on using nflreadR to read historic NFL results and model game outcomes with tidymodels.\nFirst, a quick introduction of the R packages we’ll use. nflreadR is a part of the nflverse family of packages that easily and efficiently obtain data from NFL games. This includes past games results and statistics. In this post, we’ll be using its suite of functions to get the data we need to build a simple predictive model. We’ll also use the tidymodels package to setup our model and tidyverse for data cleaning and manipulation.\n\n# install and load packages\n#install.packages(\"nflreadr\")\nlibrary('nflreadr')\nlibrary(\"tidyverse\")\nlibrary('pROC')\nlibrary('tidymodels')"
  },
  {
    "objectID": "posts/nfl-game-prediction/index.html#intro",
    "href": "posts/nfl-game-prediction/index.html#intro",
    "title": "Predicting NFL Game Outcomes w/ tidymodels & nflreadR",
    "section": "",
    "text": "Hello everyone! Today I want to share a tutorial on using nflreadR to read historic NFL results and model game outcomes with tidymodels.\nFirst, a quick introduction of the R packages we’ll use. nflreadR is a part of the nflverse family of packages that easily and efficiently obtain data from NFL games. This includes past games results and statistics. In this post, we’ll be using its suite of functions to get the data we need to build a simple predictive model. We’ll also use the tidymodels package to setup our model and tidyverse for data cleaning and manipulation.\n\n# install and load packages\n#install.packages(\"nflreadr\")\nlibrary('nflreadr')\nlibrary(\"tidyverse\")\nlibrary('pROC')\nlibrary('tidymodels')"
  },
  {
    "objectID": "posts/nfl-game-prediction/index.html#load-data",
    "href": "posts/nfl-game-prediction/index.html#load-data",
    "title": "Predicting NFL Game Outcomes w/ tidymodels & nflreadR",
    "section": "Load Data",
    "text": "Load Data\nNow that we have the relevant packages loaded, let’s get started getting our data together. Starting with game data, we’ll pull game results from 2011 - 2021. Here, we see that we get a schedule where each row (record) represents a game. There’s a home and away team, corresponding scores, and more contextual information for each game.\n\n# Scrape schedule Results\nload_schedules(seasons = seq(2011,2024)) -&gt; nfl_game_results \nhead(nfl_game_results)\n\n# A tibble: 6 × 46\n  game_id   season game_type  week gameday weekday gametime away_team away_score\n  &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;int&gt;\n1 2011_01_…   2011 REG           1 2011-0… Thursd… 20:30    NO                34\n2 2011_01_…   2011 REG           1 2011-0… Sunday  13:00    PIT                7\n3 2011_01_…   2011 REG           1 2011-0… Sunday  13:00    ATL               12\n4 2011_01_…   2011 REG           1 2011-0… Sunday  13:00    CIN               27\n5 2011_01_…   2011 REG           1 2011-0… Sunday  13:00    IND                7\n6 2011_01_…   2011 REG           1 2011-0… Sunday  13:00    TEN               14\n# ℹ 37 more variables: home_team &lt;chr&gt;, home_score &lt;int&gt;, location &lt;chr&gt;,\n#   result &lt;int&gt;, total &lt;int&gt;, overtime &lt;int&gt;, old_game_id &lt;chr&gt;, gsis &lt;int&gt;,\n#   nfl_detail_id &lt;chr&gt;, pfr &lt;chr&gt;, pff &lt;int&gt;, espn &lt;chr&gt;, ftn &lt;int&gt;,\n#   away_rest &lt;int&gt;, home_rest &lt;int&gt;, away_moneyline &lt;int&gt;,\n#   home_moneyline &lt;int&gt;, spread_line &lt;dbl&gt;, away_spread_odds &lt;int&gt;,\n#   home_spread_odds &lt;int&gt;, total_line &lt;dbl&gt;, under_odds &lt;int&gt;,\n#   over_odds &lt;int&gt;, div_game &lt;int&gt;, roof &lt;chr&gt;, surface &lt;chr&gt;, temp &lt;int&gt;, …\n\n\nWe’ve loaded our schedules in with some interesting variables to use in our model. However, it’s not quite in the format we need it to be. Ideally, we’d like to feed in 2 teams and have the model give us a winner.\n\nnfl_game_results %&gt;%\n  # Remove the upcoming season\n  filter(season &lt; 2024) %&gt;%\n  pivot_longer(cols = c(away_team,home_team),\n               names_to = \"home_away\",\n               values_to = \"team\") %&gt;%\n  mutate(team_score = ifelse(home_away == \"home_team\",yes = home_score,no = away_score),\n         opp_score = ifelse(home_away == \"home_team\", away_score,home_score)) %&gt;%  # sort for cumulative avg\n  arrange(season,week) %&gt;%\n  select(season,game_id,team,team_score,opp_score,week) -&gt; team_games\n\nLet’s use pivot_longer() to rearrange our dataset and select some simple variables before making the matchup set."
  },
  {
    "objectID": "posts/nfl-game-prediction/index.html#feature-engineering",
    "href": "posts/nfl-game-prediction/index.html#feature-engineering",
    "title": "Predicting NFL Game Outcomes w/ tidymodels & nflreadR",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nOur goal is to be able to predict the outcome of each. To do that, we need to think about what impacts the outcome of a game before teams even take the field.\nIn the case of an NFL game it could be things like player skill level, how far a team has to travel, injuries, even the food that players ate the night before. Using nflreadr we can see that there are several variables that can potentially impact the game’s outcome from injuries to previous results.\nWe’ll start off by pulling in previous_results. By using previous results, we can hopefully capture a team’s quality as a predictor for how they will perform in the next game. There are several ways to quantify team strength, some more complex than others, but for this tutorial, we will use cumulative results as a measure of team strength. The results will be in the form of cumulative points scored/allowed and winning percentage leading up to the game.\n\nteam_games %&gt;%\n  arrange(week) %&gt;%\n  group_by(season,team) %&gt;%\n  # For each team's season calculate the cumulative scores for after each week\n  mutate(cumul_score_mean = cummean(team_score),\n          cumul_score_opp = cummean(opp_score),\n          cumul_wins = cumsum(team_score &gt; opp_score),\n          cumul_losses = cumsum(team_score &lt; opp_score),\n          cumul_ties = cumsum(team_score == opp_score),\n         cumul_win_pct = cumul_wins / (cumul_wins + cumul_losses),\n         # Create the lag variable\n         cumul_win_pct_lag_1 = lag(cumul_win_pct,1),\n         cumul_score_lag_1 = lag(cumul_score_mean,1),\n         cumul_opp_lag_1 = lag(cumul_score_opp,1)\n         ) %&gt;%\n  # Non-lag variables leak info\n  select(week,game_id,contains('lag_1')) %&gt;%\n  ungroup() -&gt; cumul_avgs\n\nLet’s also calculate winning percentage as a feature.\n\nteam_games %&gt;%\n  group_by(season,team) %&gt;%\n  summarise(wins = sum(team_score &gt; opp_score),\n            losses = sum(team_score &lt; opp_score),\n            ties = sum(team_score == opp_score))%&gt;%\n  ungroup() %&gt;%\n  arrange(season) %&gt;%\n  group_by(team) %&gt;%\n  mutate(win_pct = wins / (wins + losses),\n         lag1_win_pct = lag(win_pct,1)) %&gt;%\n  ungroup() -&gt; team_win_pct\n\nThis should be a good start, but I still feel like something is missing. Football is a dangerous game and players regularly get injured. Thankfully nflreadr provides weekly injury reports. Let’s try incorporating that into our model.\n\n# Load depth charts and injury reports\ndc = load_depth_charts(seq(2011,most_recent_season()))\ninjuries = load_injuries(seq(2011,most_recent_season()))\n\n\ninjuries %&gt;%\n  filter(report_status == \"Out\") -&gt; out_inj\n\ndc %&gt;% \n  filter(depth_team == 1) -&gt; starters\n\n# Determine roster position of injured players\nstarters %&gt;%\n  select(-c(last_name,first_name,position,full_name)) %&gt;%\n  inner_join(out_inj, by = c('season','club_code' = 'team','gsis_id','game_type','week')) -&gt; injured_starters\n\n# Number of injuries by position\ninjured_starters %&gt;%\n  group_by(season,club_code,week,position) %&gt;%\n  summarise(starters_injured = n()) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from = position, names_prefix = \"injured_\",values_from = starters_injured) -&gt; injuries_position\n\nhead(injuries_position)\n\n# A tibble: 6 × 19\n  season club_code  week injured_S injured_LB injured_RB injured_T injured_C\n   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;int&gt;      &lt;int&gt;      &lt;int&gt;     &lt;int&gt;     &lt;int&gt;\n1   2011 ARI           7         1         NA         NA        NA        NA\n2   2011 ARI           8         1         NA         NA        NA        NA\n3   2011 ARI           9        NA          1          1        NA        NA\n4   2011 ARI          10         1          1         NA        NA        NA\n5   2011 ARI          11         1          1         NA        NA        NA\n6   2011 ARI          12         1          1         NA        NA        NA\n# ℹ 11 more variables: injured_DT &lt;int&gt;, injured_WR &lt;int&gt;, injured_CB &lt;int&gt;,\n#   injured_G &lt;int&gt;, injured_K &lt;int&gt;, injured_TE &lt;int&gt;, injured_QB &lt;int&gt;,\n#   injured_DE &lt;int&gt;, injured_LS &lt;int&gt;, injured_P &lt;int&gt;, injured_FB &lt;int&gt;\n\n\nAlright, now we have some flags for injured starter at each position. Next, we need to bring all of our new features together."
  },
  {
    "objectID": "posts/nfl-game-prediction/index.html#joins",
    "href": "posts/nfl-game-prediction/index.html#joins",
    "title": "Predicting NFL Game Outcomes w/ tidymodels & nflreadR",
    "section": "Joins",
    "text": "Joins\n\nnfl_game_results %&gt;%\n  inner_join(cumul_avgs, by = c('game_id','season','week','home_team' = 'team')) %&gt;%\n  inner_join(cumul_avgs, by = c('game_id','season','week','away_team' = 'team'),suffix = c('_home','_away'))-&gt; w_avgs\n\n# Check for stragglers\nnfl_game_results %&gt;%\n  anti_join(cumul_avgs, by = c('game_id','season','home_team' = 'team','week')) -&gt; unplayed_games\n\n# Join previous season's results\n#w_avgs %&gt;%\n#  left_join(team_win_pct,by = c('season','home_team' = 'team')) %&gt;%\n#  left_join(team_win_pct, by = c('away_team' = 'team','season'),suffix = c('_home','_away')) -&gt; matchups\n\n\n# Indicate whether home team won\nw_avgs %&gt;%\n  mutate(home_win = as.numeric(result &gt; 0)) -&gt; matchups\n\nNow, let’s bring in our injury data.\n\nmatchups %&gt;%\n  left_join(injuries_position,by = c('season','home_team'='club_code','week')) %&gt;%\n  left_join(injuries_position,by = c('season','away_team'='club_code','week'),suffix = c('_home','_away')) %&gt;%\n  mutate(across(starts_with('injured_'), ~replace_na(.x, 0))) -&gt; matchup_full\n\nAnd … BOOM! We have a dataset with game-by-game matchups and some features to start out. Feel free to peruse the data to find potential features to include in our Model."
  },
  {
    "objectID": "posts/nfl-game-prediction/index.html#modeling",
    "href": "posts/nfl-game-prediction/index.html#modeling",
    "title": "Predicting NFL Game Outcomes w/ tidymodels & nflreadR",
    "section": "Modeling",
    "text": "Modeling\nAhh finally, now we can get to the actual model building…. which we’ll do in about 3 lines of code.\n\nlibrary('glmnet')\n# Penalized Linear Regression\n# Mixture = 1 means pure lasso\nlr_mod &lt;- \n  logistic_reg(mixture = 0.05,penalty = 1) %&gt;% \n  set_engine(\"glmnet\")\n\nAnd that’s it! We’ll start off with a basic logistic regression. There are a few tunable (though we won’t be tuning in this post) parameters, but we’ll manually set them for this exercise.\nNext, we want to train our model with cross-validation, so that we can train and test against different samples to avoid an overfit model as much as we can.\n\n# Create folds for cross-validation\nfolds &lt;- vfold_cv(train_data)\n\nThe tidymodels workflow helps organize the steps of the model creation and consolidate model objects. We won’t go into details of workflows in this post, but there is plenty of online documentation.\n\n# create a workflow using recipe and model objects\ngame_pred_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lr_mod) %&gt;% \n  add_recipe(rec_impute)\n\n\nfit_cv = game_pred_wflow %&gt;%\n  fit_resamples(folds)\n\nCheck for best model fit. It looks like using ROC-AUC and accuracy agree that the first model is the best.\n\ncollect_metrics(fit_cv)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.623    10 0.00725 Preprocessor1_Model1\n2 brier_class binary     0.229    10 0.00100 Preprocessor1_Model1\n3 roc_auc     binary     0.719    10 0.00806 Preprocessor1_Model1\n\n\nExtract best model fit.\n\nfinal_wf = game_pred_wflow %&gt;%\n  last_fit(splits)\n\n\nfinal_model = extract_workflow(final_wf)\n\nGet variable estimates and penalty terms.\n\nfinal_model %&gt;%\n  extract_fit_engine() %&gt;%\n  tidy() %&gt;%\n  rename(penalty = lambda)\n\n# A tibble: 3,622 × 5\n   term         step estimate penalty dev.ratio\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)     1    0.253    3.74 -4.05e-14\n 2 (Intercept)     2    0.250    3.41  1.76e- 3\n 3 (Intercept)     3    0.247    3.11  4.56e- 3\n 4 (Intercept)     4    0.242    2.83  7.95e- 3\n 5 (Intercept)     5    0.237    2.58  1.15e- 2\n 6 (Intercept)     6    0.232    2.35  1.51e- 2\n 7 (Intercept)     7    0.227    2.14  1.88e- 2\n 8 (Intercept)     8    0.221    1.95  2.26e- 2\n 9 (Intercept)     9    0.215    1.78  2.65e- 2\n10 (Intercept)    10    0.209    1.62  3.04e- 2\n# ℹ 3,612 more rows\n\n\n\n# Align predictions to test dataset\npredicted_df = augment(final_model,test_data) \n\nNow, so we get a sample boost, let’s retrain the model on the full training sample.\n\n# Extract model specs\nfinal_model %&gt;%\n  extract_spec_parsnip() -&gt; final_specs\n\n# Update workflow object and retrain on full training set w/ same parameters\ngame_pred_wflow %&gt;%\n  update_model(spec = final_specs) %&gt;%\n  # Bind test and training data together\n  fit(data = cbind(train_data,test_data))-&gt; final_flow\n\nLet’s now save the model for future use and investigation.\n\nlibrary(yaml)\n\nfinal_flow %&gt;%\n  extract_fit_parsnip() %&gt;%\n  saveRDS(file = \"gamePred_model2024.rda\")\n\nWe’ll use this model to make predictions on future games and evaluate how our model performs in real-time."
  },
  {
    "objectID": "posts/archive/2020 NBA Draft Clustering.html",
    "href": "posts/archive/2020 NBA Draft Clustering.html",
    "title": "Data-Driven Descriptions: Using Machine Learning to Profile 2020 NBA Draft Prospects",
    "section": "",
    "text": "If you have ever read, listened to, or watched analysis of an NBA draft you might have heard some strange sounding phrases like “3 – and – D wing”, “rim-protector”, “pure scorer”, “raw athlete”, and “playmaker”  used to describe a player. What does that mean? It seems like these adjectives are all meant to do one thing - “profile” a player. Profiles are a quick summary or composite description of who a player is, expressing their playing style, strengths, weaknesses, role on a team, etc. From a fan’s perspective, these descriptions give us an idea of the player’s projected role on an NBA team without having to go back and watch hours of the player’s games. In addition, it might help us fans identify players that fit a need on our team. For example, if you’re a Cavs or Blazers fan, a solid perimeter defender may be what you’re looking for. If you’re a Sixers or Thunder fan, you might be interested in knowing the sharpshooters that could help your team. As a fan of both college basketball and the NBA Draft, these profiles and descriptions are intriguing, and I’d love to take a crack at developing my own. The only problem is that I haven’t watched nearly enough college hoops, nor have I been paying enough attention to this year’s prospects. Maybe I should leave the analysis to actual analysts on TV, but part of me still thinks I can come up with useful player profiles using data. In this post, I’ll attempt to address the idea of “profiling” NBA draft prospects (specifically, guards), using an unsupervised machine learning technique called hierarchical clustering and the season statistics of NCAA prospects dating back to 2011.\n \n\nWhat is Clustering?\n\nThe reason I am using hierarchical clustering is because the clusters that a player is assigned to can reveal the defining characteristics of that player. Without getting too technical, hierarchical clustering is an unsupervised machine learning technique used to divide observations in a dataset into clusters or groups based on statistical similarity. By grouping similar players together and evaluating the groups, we can generalize the qualities of players in each group. For example, one group may consist of players with a high 3-point percentage and low assist percentage revealing that they were generally effective as off-ball shooters for their college team. Of course, not all players have skill sets that can be identified with a given set of statistics or any available statistics for that matter. However, that is one of the challenges of generalizing player profiles. In this analysis, I do my best to mitigate these issues, but there are still a few players whose resulting profiles don’t make a ton of sense.\nUnlike supervised models, trying to find out if the results of a cluster analysis are “good” doesn’t come down to prediction accuracy or error, but rather how similar observations are to others in their cluster and how different they are from those outside. There are ways to validate resulting clusters, like using Silhouette scores or Dunn’s Index, that judge if the resulting groups actually contain mathematically similar observations. If this is getting too technical, don’t worry, the bottom line is that players that are grouped together should generally share more in common then players not grouped together.\nHowever, for this post, mathematically “good” results will not be prioritized over interpretability. In fact, my criteria for success is not technical at all. In order for this analysis to be a success, the resulting clusters/groups must be interpretable and must not be indicative of NBA success. That means that each group of players should have defining characteristics that can be described using basketball terminology and that each group includes players with varying levels of NBA success. Again, the goal of this analysis is build profiles that describe a player, not predict his chances of success. By the way, if you haven’t already noticed I use “cluster” and “group” interchangeably, sorry for any confusion but they mean the same thing.\n\nApproach and Data\n\nRather than using only the college statistics of the 2020 class, I am including the college stats of current NBA players to make the resulting groups more interpretable. Of course, I will be using the average statistics of each group to get a better understanding of the group’s defining traits. However, by including current NBA players in the analysis, each group’s characteristics become more recognizable. For example, any group headlined by Buddy Hield or Joe Harris could probably be identified as a group consisting of primarily sharpshooters while a group including Matisse Thybulle and Marcus Smart could be thought of as a group of good on-ball defenders. In addition, the model does not consider the year that each player was drafted meaning that Anthony Edwards, Tre Jones, and Cole Anthony will be grouped together with similar players from previous draft classes. As a result, we’ll also get an idea of each 2020 draft class member’s NBA comparisons. Of course, there are always going to be players whose college profile is different from their NBA profile, but that doesn’t seem to affect the results too much.\nIf you want to check out the technical details/data selection, it will be available on GitHub. Long story short, I used the statistics of the final college year of almost every single guard that has played in the NBA since 2011 as well as guards included in NBADraftNet.com’s 2020 rankings. Unfortunately, this analysis does not include players that did not play in the NCAA. That means no LaMelo Ball, Killian Hayes, Theo Maldeon, or RJ Hampton nor does the analysis include any current players that played overseas instead of in college. That means players like Bogdan Bogdanovic, Dennis Schroder, or Emmanuel Mudiay will be left out as well.\nNext, as un-inspiring as it sounds, I decided to hand-pick statistics that I felt were most important when trying to discern the various roles and playing styles of guards among each other. The final set of statistics I used were assist percentage (AST %), usage percentage (USG%), three-point attempt rate (3PAr), effective-field goal percentage (EFG%), and defensive box-plus-minus (DBPM).\n \n\nResults\n\nThe final model produced 6 clusters of players that generally make sense and can serve as statistical profiles. It was by no means a complete success as there were some players in questionable/interesting groups, but, as a whole, it wasn’t too difficult to come up with descriptions for each cluster using group averages and the players within them. Here is an overview of each group’s statistical profile.\n \n\nGroup 1: Low Efficiency 3 – and – D\n\nIn our first group, players tended to be solid defenders, but weren’t very efficient scorers, nor were they the primary creators for their team. The group includes notable NBA starters like Donovan Mitchell, Gary Harris, and Kentavious Caldwell-Pope, but also contained players that didn’t make much of an impact at the next level like Rawle Alkins, Aaron Harrison, and Malachi Richardson. The best-case scenarios for these guys don’t look too bad. Harris and KCP were both starters for the two Western Conference Finals teams with KCP going on to win the finals with LA as a key contributor on both ends. He was also part of one of the best defensive units in the league. It’s also surprising that Donovan Mitchell was included in this group. He has now become the offensive focal point of the Utah Jazz and an All-Star in the process of shedding this label. It’s also worth noting that all of the top 3 players are not known to be particularly lethal shooters, but tend to be streaky shooters capable of going on hot/cold stretches at a moment’s notice. Streaky shooters are notorious for their willingness to shoot despite their recent struggles. Therefore, the clustering did a good job of grouping players that will continue to exhibit a high three-point attempt rate regardless of the percentage they are shooting.\nThe 2020 prospect that fell into this group was Isaiah Joe, a 6’5 guard from Arkansas whose efficiency (49.7 EFG%) isn’t great, but he did shoot a lot of threes (76.4 % 3PTAr).\n \n Statistical profile of players in Cluster/Group 1\n\nGroup 2: Old-school floor generals\n\nFor group 2, we can find offensively efficient primary ballhandlers/creators given the groups relatively high effective field goal percentage, low usage percentage, and high assist percentage. Players in this group include Denzel Valentine, Derrick White, and Reggie Jackson as well as Scott Machado and Ray MacCallum. These players really made sense when looking at the group averages. They seemed to be making good decisions with the basketball, assisting a large amount of teammate field goals while using up a relatively small share of possessions (turnovers are also included in usage percentage). In addition, despite their lack of three-point shooting, they still shot the ball very efficiently inferring that they took smart shots and often found higher percentage looks. While no one assigned to this group is a star, there are still solid role players and starters in the NBA that carried this label in college.\nThe only 2020 prospect assigned to this group was Oregon’s Payton Pritchard. who shot threes at a decently high rate (45.9 % as a senior) along with solid efficiency numbers.\n Statistical profile of players in Cluster/Group 2\n \n \n\nGroup 3 – High Volume Scorers\n\nIn group 3, we primarily found what some might call volume scorers. These players had a high usage rate and a low assist percentage suggesting that they used a large portion of their team’s possessions to shoot or turn it over. They also carried okay scoring efficiency and subpar defense. Notable NBA players include Buddy Hield, Damian Lillard, and Jamal Murray while fringe players include Xavier Munford, Rashad Vaughn, and Gian Clavall. It’s important to note that while the statistical profiles of players in this group don’t seem great, some of them have still gone on to become solid NBA contributors. Damian Lillard has become a superstar and can get quality shots from almost anywhere on the court. Hield, despite his high volume at Oklahoma, was still a very efficient shooter (0.623 EFG%) and was a key contributor for Sacramento before issues with the coaching staff. Finally, Jamal Murray exploded onto the scene in this year’s playoffs helping Denver to the Western Conference Finals with a ridiculous 62.6 True Shooting percentage and a stretch of 3 games in which he scored a total of 142 points.\nThe 2020 prospects assigned to this group were Markus Howard (Marquette) who has a high usage rate (39.3), low DBPM (0.6) and decent efficiency (53 EFG%) and Anthony Edwards (Georgia: we’ll get to him later).\n Statistical profile of players in Cluster/Group 3\n \n \n\nGroup 4 – Focal Points (… No pun intended)\n\nGroup 4 players looked clearly like high offensive load bearers as they had high usage and assist percentages. That combination signifies that much of the offense ran through them as they worked as the primary facilitators and shot at high volumes. Also, these guards didn’t take many threes, weren’t super-efficient, nor had great defensive numbers. Notable NBA include Trae Young, DeAngelo Russell, Ja Morant, Klay Thompson, and Dejounte Murray and fringe players Walt Lemon, Milton Doyle, and Mike James. Now you may be questioning Trae Young and Klay’s inclusion in this group, but both carried high offensive loads and weren’t that efficient, the only difference is that their three-point attempt rates were very high.\nNevertheless, what you can take away from this group is that it’s best players have no issue handling the scoring and creation responsibilities at the next level. Trae Young and DeAngelo Russell are already All-Stars with high usage and assist rates while Ja Morant seems to be following a similar trajectory in Memphis. Maybe if a player like this is given the reigns on a team in need of someone to shoulder that load, they can thrive.\n2020 prospects include: Cassius Winston (Michigan State), Saben Lee (Vanderbilt), Jamil Wilson (Marquette), Cole Anthony (UNC), and Grant Riller (Charleston).\n Statistical profile of players in Cluster/Group 4\n\nGroup 5 – Lockdown Combo Guards\n\nGroup 5 consists primarily of defensive specialists who were also the primary offensive facilitators on their team. Players of this group generally have high DBPM, and high AST% while not being the most efficient shooters. Notable NBA players include Marcus Smart, De’Aaron Fox, Shai Gilgeous-Alexander, Delon Wright, Matisse Thybulle, and Malcom Brogdon while fringe players include Tyrone Wallace, Travon Duval, Troy Caupain. There seem to be players like this available all over the draft from pick # 5 (Fox, Smart) to pick # 36 (Brogdon).  I am a big fan of this group because it contains many solid, underrated players. Shai Gilge….. is a fun player to watch and might become the cornerstone for the Thunder franchise. Marcus Smart is such a good defender that there was an argument that he should’ve been the Defensive Player of the Year. Finally, 2020 prospects to watch are Devon Dotson and Malachi Flynn who seemed to fit this statistical description pretty well.\nThere were quite a few 2020 prospects assigned to this group including Ashton Hagans (Kentucky), Devon Dotson (Kansas: 4.8 DBPM), Malachi Flynn (4.1 DBPM, San Diego State), Josh Green (Arizona), Tre Jones (Duke), and Tyrese Maxey (Kentucky: 1.45 AST:TO Ratio).\n Statistical profile of players in Cluster/Group 5\n\nGroup 6 – Efficient 3 – and – D Wings\n\nFinally, players of Group 6 look like they can be true 3-and-D wings. These players had very high EFG% to go with a high 3PAr. They also carried a decent DBPM while carrying low usage and assist rates. Notable NBA players include Bradley Beal, Devin Booker, Joe Harris, Tyler Herro, Terrance Ross, as well as Lonzo Ball and Victor Oladipo (64.8% EFG, 6.2 DBPM). This may seem strange, but Lonzo Ball was very efficient as a shooter (66.8 % EFG), shot lots of 3s (56.6 % 3PAr), and was a great defender (3.9 DBPM), he just happened to also have a very high assist percentage (31.4%). Overall, these also seem to be the most “NBA ready”  players in the draft. Most of the top players of this group were starters right away. Most recently and perhaps notably, 19 year old Tyler Herro started every game for the Eastern Conference champion Miami Heat. This early success might be due to the shift of the game as a whole. As teams have started embracing the three-point shot as more of a necessity rather than an option, players who are good 3-point shooters have naturally become more valuable in today’s game.\n2020 prospects include Tyrese Haliburton (Iowa State), Tyrell Terry (Stanford: 45.6% 3Par, 20 AST%, 53.5% EFG), Immanuel Quickly (Kentucky), Desmond Bane (TCU), and Cassius Stanley (Duke).\n Statistical profile of players in Cluster/Group 6\n\nConclusion\n\nThere are a few points to mention with these results. First, It looks like there are plenty of solid defenders to be found in this upcoming draft (both primary ballhandlers and shooters) by the large representation of 2020 prospects found in clusters 5 and 6. Second, some notable players I want to analyze further include Anthony Edwards and Tyrese Haliburton.\nEdwards was placed in a group occupied primarily by volume scorers. He will be a top-3 pick, but will teams consider his lack of defensive impact (0.7 DBPM) and low efficiency (47.3% EFG)? Of course, players can improve and maybe the top of the draft is a perfect spot for high usage, low efficiency “projects”. Teams at the top of the draft may be more willing to give a longer leash to prospects and being on a bad team might give Edwards opportunities, in terms of volume, that could help him develop. Just look at fellow top-10 picks in his group – Damian Lillard, Buddy Hield, Jamal Murray, Brandon Knight, Austin Rivers. While Edwards’ future team hopes it’s not the latter two (By the way, Knight had a promising start to his career before injuries got in the way), the first three may be indicators of how his team should handle his development. For this reason, perhaps Minnesota, a team hoping to make a push for the playoffs and maximize the opportunity they have with KAT and DeAngelo Russell, should opt for someone who will not command a high volume. Honestly, the same can be said for Golden State, Edwards will likely have to cede volume to Klay, Steph, Draymond, and Andrew Wiggins and will also be expected to contribute immediately to a deep playoff push next year. I could see Charlotte, despite solid guard play from Devonte Graham and Terry Rozier this season, being a good fit for Edwards. They don’t seem close to competing for anything just yet and could be the perfect landing spot for Edwards to get the opportunities he needs to develop.\nIn addition, the inclusion of Tyrese Haliburton as a 3 – and – D wing is also interesting. He has a high assist percentage (35%) and effective field-goal percentage (61.1%) while carrying a relatively low usage rate for a point guard (20.1%) and shot about half of his shots (50.8%) from downtown. This might mean that he has a versatile skill set and could serve a team in multiple ways in the NBA. For teams that already have young point guards like Chicago or New York, Haliburton might still be a good fit for operating in some sort of hybrid role. Even teams with a perceived need for a point guard like Detroit or Phoenix, could use him as their primary ballhandler.\nIn conclusion, the groups produced by the hierarchical clustering model met the goals originally defined for them: they were interpretable, and each cluster contained players with different levels of NBA success. Of course, the results weren’t perfect as a few players seemed to be placed in groups unintuitively, but not all basketball players can be easily profiled with a small set of statistics.  Nevertheless, I hope that this post provided a new perspective on player profiling using a more statistical approach.\nAll data used for this project was obtained from Basketball-Reference.com."
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html",
    "href": "posts/post-with-code/wr_fantasy_pts.html",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "",
    "text": "I’ve played fantasy football for a decade and have always wondered how ESPN (and Yahoo, Sleeper, etc.) make their fantasy projections week to week. Where are they getting their estimates from? Why is Ja’Marr Chase projected to score 16.3 points for me this week? Why does his projection increase by that much (or that little) if Tee Higgins happens to be out injured? Perhaps we won’t get a chance to peak inside of ESPN’s crystal ball, but maybe we can try to build our own!\nIn this post, I’ll attempt to build a predictive model that can output our projections for fantasy football players based on their past performance. We’ll narrow the scope to wide receivers now, but this process can largely be applied to players of any position.\nWe’ll go step by step through data cleaning, feature engineering, model selection and training, and model validation before finishing up with conclusions and takeaways."
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#introduction",
    "href": "posts/post-with-code/wr_fantasy_pts.html#introduction",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "",
    "text": "I’ve played fantasy football for a decade and have always wondered how ESPN (and Yahoo, Sleeper, etc.) make their fantasy projections week to week. Where are they getting their estimates from? Why is Ja’Marr Chase projected to score 16.3 points for me this week? Why does his projection increase by that much (or that little) if Tee Higgins happens to be out injured? Perhaps we won’t get a chance to peak inside of ESPN’s crystal ball, but maybe we can try to build our own!\nIn this post, I’ll attempt to build a predictive model that can output our projections for fantasy football players based on their past performance. We’ll narrow the scope to wide receivers now, but this process can largely be applied to players of any position.\nWe’ll go step by step through data cleaning, feature engineering, model selection and training, and model validation before finishing up with conclusions and takeaways."
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#r-markdown",
    "href": "posts/post-with-code/wr_fantasy_pts.html#r-markdown",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "R Markdown",
    "text": "R Markdown\n\nlibrary(tidyverse)\nlibrary(nflreadr)\nlibrary(tidymodels)\nlibrary(nflfastR)\nlibrary(naniar)\nlibrary(TTR)"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#data-load",
    "href": "posts/post-with-code/wr_fantasy_pts.html#data-load",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "Data Load",
    "text": "Data Load\nWe’ll start by loading in directly relevant player statistics and depth chart information before joining them together.\n\nstats = load_player_stats(seasons = seq(2006,2023))\n\ndc = load_depth_charts(season = seq(2016,most_recent_season())) %&gt;% filter(position == 'WR',formation == 'Offense') %&gt;%\n  select(season,recent_team = club_code,week,season_type = game_type,player_id = gsis_id,depth_team)\n\n\n# Filter for wide receiver plays\nwr_data &lt;- stats %&gt;%\n  filter(position == \"WR\") %&gt;%\n  # Select identifying info + receiver specfic metrics\n  select(player_id,player_name,position,recent_team,season,week,season_type,\n         receptions:fantasy_points_ppr) %&gt;%\n  # Add depth chart status since we don't have participation data\n  left_join(y = dc,by = c('player_id','week','season','season_type',\"recent_team\")) %&gt;%\n  # Only first 3 are counted so some players are NA'd if they're below 3rd on DC\n  replace_na(list(depth_team = '4')) %&gt;%\n  mutate(depth_team = as.numeric(depth_team))"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#missingness",
    "href": "posts/post-with-code/wr_fantasy_pts.html#missingness",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "Missingness",
    "text": "Missingness\nLet’s check if there are any missing values in the dataset we’ve created. We’ll use the library naniar to do so.\n\n# Explore variables patterns of missingness\ngg_miss_var(wr_data)\n\n\n\n\n\n\n\nwr_data %&gt;%\n  select(racr,air_yards_share,wopr,target_share,receiving_epa) -&gt; df_miss\n\ndf_miss %&gt;%\n  gg_miss_upset()\n\n\n\n\n\n\n\n# What if we take out racr?\n\nIdeally, we would like to impute these values, but for brevity, we’ll filter out the NA’s of ‘racr’."
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#time-weighting",
    "href": "posts/post-with-code/wr_fantasy_pts.html#time-weighting",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "Time Weighting",
    "text": "Time Weighting\nWe want our predictions for each player to be based on their past performance. However, we’d like the most recent performances to be weighed heaviest. To do that, we can introduce a simple time-weighting scheme. In the function below, we take a data vector (ex. receptions) and return a weighted average based on the length of the vector.\n\n# Create Time Weighting \n\nweighted_avg = function(metric_vector){\n  \n  # Take in sliding window of vector of chosen metric\n  n = length(metric_vector)\n\n  # Create Weights for each value based on recency\n  weights = seq(1,n)\n  \n  # Calculated weighted average\n  w_avg = sum(metric_vector * weights) / sum(weights)\n  \n  return(w_avg)\n\n}\n\nNow that we have our simple time-weighting built, let’s make use of Davis Vaughn’s slider package. It’s a tidy way to calculate sliding window summaries. We’ll do that to create a set of potential numeric predictors.\n\nlibrary(slider)\n\nwr_data %&gt;%\n  # Should remove most missingness\n  filter(!is.na(racr)) %&gt;%\n  group_by(player_id,season) %&gt;%\n  arrange(week) %&gt;%\n  # Weighted Avg (moving)\n  # Take lag so we are not leaking data\n  mutate(across(receptions:depth_team, ~ lag(slide_dbl(.x,.f = weighted_avg,.before = Inf,.complete = TRUE)),.names = \"wt_{col}\")) %&gt;%\n  ungroup() %&gt;%\n  # Convert negative fantasy points to 0\n  mutate(fantasy_points_target = ifelse(fantasy_points &lt; 0,0,fantasy_points),\n         # Take log (more on this later)\n         log_fantasy_points = log(fantasy_points_target + 1)) -&gt; ma_wr\n\nLet’s use the example of Brandon Aiyuk as a sanity check.\n\nma_wr %&gt;%\n  filter(season == 2023, player_id == '00-0036261') %&gt;%\n  select(player_name,week,targets,wt_targets, receptions,wt_receptions, receiving_yards,wt_receiving_yards)\n\n# A tibble: 19 × 8\n   player_name  week targets wt_targets receptions wt_receptions receiving_yards\n   &lt;chr&gt;       &lt;int&gt;   &lt;int&gt;      &lt;dbl&gt;      &lt;int&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n 1 B.Aiyuk         1       8      NA             8         NA                129\n 2 B.Aiyuk         2       6       8             3          8                 43\n 3 B.Aiyuk         4       6       6.67          6          4.67             148\n 4 B.Aiyuk         5       7       6.33          4          5.33              58\n 5 B.Aiyuk         6      10       6.6           4          4.8               76\n 6 B.Aiyuk         7       6       7.73          5          4.53              57\n 7 B.Aiyuk         8       9       7.24          5          4.67             109\n 8 B.Aiyuk        10       3       7.68          3          4.75              55\n 9 B.Aiyuk        11       6       6.64          5          4.36             156\n10 B.Aiyuk        12       4       6.51          2          4.49              50\n11 B.Aiyuk        13       7       6.05          5          4.04              46\n12 B.Aiyuk        14       9       6.21          6          4.20             126\n13 B.Aiyuk        15       5       6.64          3          4.47              37\n14 B.Aiyuk        16       7       6.41          6          4.26             113\n15 B.Aiyuk        17       8       6.49          7          4.50             114\n16 B.Aiyuk        18       4       6.68          3          4.81              25\n17 B.Aiyuk        20       6       6.36          3          4.60              32\n18 B.Aiyuk        21       8       6.32          3          4.42              68\n19 B.Aiyuk        22       6       6.50          3          4.27              49\n# ℹ 1 more variable: wt_receiving_yards &lt;dbl&gt;\n\n\nA quick scan shows that our time weighted averages are calculated as intended."
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#correlations",
    "href": "posts/post-with-code/wr_fantasy_pts.html#correlations",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "Correlations",
    "text": "Correlations\nBefore we start splitting data for the model, let’s take a look at variable correlations. This may influence our model choices.\n\n# Calculate the correlation matrix\ncor_matrix &lt;- ma_wr %&gt;% \n  select(starts_with(\"wt_\"), fantasy_points_target) %&gt;%\n  cor(use = \"complete.obs\")\n\n# Reshape the correlation matrix for ggplot\ncor_data &lt;- reshape2::melt(cor_matrix)\n\nggplot(data = cor_data, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1,1), name=\"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n  labs(title = \"Correlation Matrix of Dataset\", x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nOkay, the heatmap here shows a lot of red. It looks like there is a high degree of multicollinearity between our weighted measures. We’ll touch on this issue, but won’t fully address it in this post. Just be aware that it can make inference and interpretation of a linear model quite difficult.\nIn addition, if we look at our target variable, we notice something interesting… it has a skewed distribution. What does this tell us about using a linear model?\n\nma_wr %&gt;%\n  ggplot(aes(x = fantasy_points_target)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLet’s see what happens if we transform our target with a log-transform.\n\nma_wr %&gt;%\n  ggplot(aes(x = log_fantasy_points)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nHmmm, our histogram still looks a bit funky. The distribution seems to have shifted closer to a bell curve we’d want for linear regression, but there are A LOT of 0s (more on that later). For now, we’ll trudge ahead and start preparing the modeling pipeline in tidymodels with this transformed variable as our target."
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#preprocess",
    "href": "posts/post-with-code/wr_fantasy_pts.html#preprocess",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "Preprocess",
    "text": "Preprocess\nWe’ll start by splitting our dataset into training and testing splits and create recipes for preprocessing in the following blocks.\n\nlibrary(tidymodels)\n\n# Split\n\nset.seed(222)\n# Put 3/4 of the data into the training set \ndata_split &lt;- ma_wr %&gt;% \n  # Filter on relevant columns\n  select(starts_with('wt_'),fantasy_points_target,player_id,season,week,recent_team,\n         fantasy_points,fantasy_points_ppr,log_fantasy_points) %&gt;% \n  # make split\n  initial_split( prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n\ntest_data %&gt;%\n  filter(!is.na(wt_receptions)) -&gt; test_data\n\nsum(colSums(is.na(test_data)))\n\n[1] 0"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#model-building--",
    "href": "posts/post-with-code/wr_fantasy_pts.html#model-building--",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "Model Building ——————————-",
    "text": "Model Building ——————————-\n\nexp_recipe = train_data  %&gt;%\n  recipe(log_fantasy_points ~ .,) %&gt;%\n  update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr,fantasy_points_target),new_role = 'ID') %&gt;%\n  # Generally not recommended to throw out all data, but for brevity, let's remove NAs\n  #step_naomit(all_numeric_predictors()) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n\n # Remove zero variance predictors (ie. variables that contribute nothing to prediction)\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_numeric_predictors())\n\n\n#summary(exp_recipe)"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#create-model",
    "href": "posts/post-with-code/wr_fantasy_pts.html#create-model",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "Create Model",
    "text": "Create Model\nCreate specifications for elastic net model using glmnet and tune"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#glm",
    "href": "posts/post-with-code/wr_fantasy_pts.html#glm",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "GLM ————————–",
    "text": "GLM ————————–\nWe’ll train a general linear model (GLM). This is a type of linear regression which provides a type of built-in variable selection. The nuts and bolts of this modeling strategy are beyond the scope of this post, but for those interested in learning, there’s tons of material on a concept called regularization upon which this strategy is built.\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\n#library(lightgbm)\n#library(bonsai)\n\nglm_spec &lt;- linear_reg(\n  penalty = tune(),     # Lambda (regularization strength)\n  mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)\n    \n) %&gt;%\n  set_engine(\"glmnet\")\n\nglm_wflow &lt;-\n  workflow() %&gt;% \n  add_recipe(exp_recipe) %&gt;%\n  add_model(glm_spec)\n\n\nwr_folds &lt;- vfold_cv(train_data, v = 5)\n\n# Tune the hyperparameters using a grid of values\nglm_tune_results &lt;- tune_grid(\n  glm_wflow,\n  resamples = wr_folds,\n  grid = 10   # Number of tuning combinations to evaluate\n)\n\nTune Model w/ Cross Validation\nExamine Tuning Results\n\n# Display tuning results\n\nglm_tune_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  arrange(mean)\n\n# A tibble: 10 × 8\n    penalty mixture .metric .estimator  mean     n std_err .config              \n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1 2.00e- 7  0.574  rmse    standard   0.799     5 0.00334 Preprocessor1_Model06\n 2 1.36e- 6  0.840  rmse    standard   0.799     5 0.00334 Preprocessor1_Model09\n 3 2.26e- 5  0.352  rmse    standard   0.799     5 0.00336 Preprocessor1_Model04\n 4 2.32e- 9  0.499  rmse    standard   0.799     5 0.00333 Preprocessor1_Model05\n 5 1.53e- 8  0.0923 rmse    standard   0.799     5 0.00335 Preprocessor1_Model01\n 6 2.89e-10  0.801  rmse    standard   0.799     5 0.00333 Preprocessor1_Model08\n 7 4.89e- 4  0.909  rmse    standard   0.799     5 0.00333 Preprocessor1_Model10\n 8 9.91e- 3  0.651  rmse    standard   0.800     5 0.00331 Preprocessor1_Model07\n 9 5.00e- 2  0.237  rmse    standard   0.800     5 0.00332 Preprocessor1_Model02\n10 3.95e- 1  0.326  rmse    standard   0.816     5 0.00272 Preprocessor1_Model03\n\n\nFind the best parameters of the group. finalize_workflow() will choose the model with the optimal set of hyperparameters as found in select_best().\n\n# Select the best hyperparameters based on RMSE\nbest_glm &lt;- select_best(glm_tune_results, metric = 'rmse')\n\n# Finalize the workflow with the best hyperparameters\nfinal_glm_workflow &lt;- finalize_workflow(glm_wflow, best_glm)\n\nbest_glm\n\n# A tibble: 1 × 3\n      penalty mixture .config              \n        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.000000200   0.574 Preprocessor1_Model06\n\n\n^ Above you’ll find the optimal configuration of the model. We won’t get too far into the weeds here, but will not that the penalty term is small (low regularization, high complexity) and the mixture (type of regularization) indicates more of a Ridge regression (more shrinkage, less variable elimination).\nBelow, we’ll take the best model and fit it to the entire training data set before validating it against the test set.\n\n# Fit the finalized model on the entire training data\nfinal_glm_fit &lt;- fit(final_glm_workflow, data = train_data)"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#model-evaluation",
    "href": "posts/post-with-code/wr_fantasy_pts.html#model-evaluation",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nDiagnostics\nNow that we have our model fit on the full training set, let’s evaluate it, check it’s reliability and it’s compliance with key assumptions.\n\n# Make predictions on the test set and tidy\nglm_predictions &lt;- augment(final_glm_fit, new_data = test_data) %&gt;%\n  mutate(.pred_fp = exp(.pred) + 1,\n         .resid = fantasy_points - .pred_fp)\n\n# Evaluate the model's performance (RMSE)\nglm_metrics &lt;- glm_predictions %&gt;%\n  metrics(truth = fantasy_points, estimate = .pred_fp)\n\n# Print the evaluation metrics\nprint(glm_metrics)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       5.45 \n2 rsq     standard       0.148\n3 mae     standard       4.21 \n\n# Distribution of predictors\nglm_predictions  %&gt;%\n  ggplot(aes(x = .resid)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n# Residuals vs fitted\nggplot(glm_predictions,aes(x = .pred_fp, y=.resid)) +\n  geom_point() +\n  geom_smooth(se = F) +\n  geom_hline(yintercept = 0,linetype = \"dashed\",color = \"red\")+\n  labs(title = \"Residuals vs. Fitted\", y= \"Residuals\",x = \"Fitted\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\n\nggplot(glm_predictions,aes(sample = .resid)) +\n  geom_qq() +\n  geom_qq_line() +\n  labs(title = \"Q-Q Plot of Residuals\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\nWhile the histogram of residuals looks relatively normal. The other plots draw attention to some potential issues. Starting with the residuals vs. fits, we’re seeing the points tail off on the right side. These points should be scattered randomly we can see a pattern. Similarly, with the QQPlot, the right side of the plot seems to tail well off course. It suggests first that the model struggles mightly with estimating large fantasy point values, it could be due to outliers and/or that a linear model may not be the best approach for this problem. This lines up with the inflated number of 0s seen in our target variable. We probably should not trust this model for valid inference or prediction and should continue to explore non-linear options or modifications to this model (i.e. variable interactions or transformations).\n\n\nVariable Importance\n\nfinal_glm_fit %&gt;% \n  vip::vi() %&gt;%\n  mutate(Importance = abs(Importance),\n         Variable = fct_reorder(Variable,Importance)) %&gt;%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col() +\n  scale_x_continuous(expand = c(0,0)) +\n  labs(x = \"Impact\")\n\n\n\n\n\n\n\n\nI’m personally not a huge fan of variable importance plots because they don’t communicate the actual “importance” of the variable to the outcome of our target. In other words, they’re not that useful and can be misleading (by themselves) for inference. They just measure the impact of the variables on predictions. I’ll take a few variables on this plot as examples.\nIn this case, we can see that “wt_target_share” is defined as “Player’s share of team receiving targets in this game”.\nIt makes sense that this has a positive impact on fantasy points scored as it directly describes fantasy opportunity for receivers. All else being equal, it’s likely that increased opportunity results in increased fantasy point value.\nHowever, does “receiving_fumbles_lost” having a positive impact on projected fantasy points seem reasonable? Considering that fantasy points are deducted when a player fumbles the ball away in real life, this doesn’t make much sense. We’d be hard-pressed to convince anyone to target a player who fumbles the ball often. Why is this the case?\nIt’s likely due to the afformentioned multicollinearity issue. In order to fumble the ball a player must first have received it. The players with the highest number of fumbles are likely the players that receive the ball most often (players that the offense wants to get the ball to) and consequently get increased opportunity to score more points in addition to fumbles. Below we can see “fumbles lost” as a function of receptions and target share, respectfully.\n\nggplot(train_data, aes(x = wt_receptions, y = wt_receiving_fumbles_lost)) + \n  geom_smooth() +\n  theme_minimal() +\n  labs(y = \"Fumbles Lost\", x = \"Receptions\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 2693 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\nggplot(train_data, aes(x = wt_target_share, y = wt_receiving_fumbles_lost)) + \n  geom_smooth() +\n  labs(y = \"Fumbles\", x= \"Target Share\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 2693 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\nAs mentioned earlier, the regularized model handles variable selection but does not completely remove the effects of multicollinearity. Multicollinearity makes inference very challenging and we will likely need a model better suited for non-linear relationships or to include more interactions to improve model performance and interpretability."
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#conclusion",
    "href": "posts/post-with-code/wr_fantasy_pts.html#conclusion",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "Conclusion",
    "text": "Conclusion\nThis was a good first step in creating a model for my fantasy football needs. There is clearly room for improvement as far as model selection goes, but it was fun to use the slider package for the first time and attempt to estimate a transformed target variable. I have some ideas for additions to and subtractions from this model that will hopefully make it into another blog post. In the meantime, big shoutout to the nflreadr team for making this data easily available.\n\n\nHo T, Carl S (2024). nflreadr: Download ‘nflverse’ Data. R package version 1.4.1.05, https://github.com/nflverse/nflreadr, https://nflreadr.nflverse.com."
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#model-building",
    "href": "posts/post-with-code/wr_fantasy_pts.html#model-building",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "Model Building",
    "text": "Model Building\n\nexp_recipe = train_data  %&gt;%\n  recipe(log_fantasy_points ~ .,) %&gt;%\n  update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr,fantasy_points_target),new_role = 'ID') %&gt;%\n  # Generally not recommended to throw out all data, but for brevity, let's remove NAs\n  #step_naomit(all_numeric_predictors()) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n\n # Remove zero variance predictors (ie. variables that contribute nothing to prediction)\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_numeric_predictors())\n\n\n#summary(exp_recipe)"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#cross-validation-tuning--",
    "href": "posts/post-with-code/wr_fantasy_pts.html#cross-validation-tuning--",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "Cross-Validation + Tuning ————-",
    "text": "Cross-Validation + Tuning ————-\n\nGLM ————————–\nWe’ll train a general linear model (GLM). This is a type of linear regression which provides a type of built-in variable selection. The nuts and bolts of this modeling strategy are beyond the scope of this post, but for those interested in learning, there are tons of material on a concept called regularization upon which this strategy is built.\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\nglm_spec &lt;- linear_reg(\n  penalty = tune(),     \n  mixture = tune(),    # Mixutre (0 = Ridge, 1 = Lasso, values in between = Elastic Net)\n    \n) %&gt;%\n  set_engine(\"glmnet\")\n\n# Setup workflow\nglm_wflow &lt;-\n  workflow() %&gt;% \n  add_recipe(exp_recipe) %&gt;%\n  add_model(glm_spec)\n\n# Create cross validation splits\nwr_folds &lt;- vfold_cv(train_data, v = 5)\n\n# Tune the hyperparameters using a grid of values\nglm_tune_results &lt;- tune_grid(\n  glm_wflow,\n  resamples = wr_folds,\n  grid = 10   # Number of tuning combinations to evaluate\n)\n\nExamine Tuning Results\n\n# Display tuning results\n\nglm_tune_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  arrange(mean)\n\n# A tibble: 10 × 8\n    penalty mixture .metric .estimator  mean     n std_err .config              \n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1 8.38e- 7   0.469 rmse    standard   0.799     5 0.00167 Preprocessor1_Model05\n 2 1.44e- 5   0.744 rmse    standard   0.799     5 0.00165 Preprocessor1_Model08\n 3 4.65e- 6   0.670 rmse    standard   0.799     5 0.00164 Preprocessor1_Model07\n 4 1.18e- 9   0.352 rmse    standard   0.799     5 0.00167 Preprocessor1_Model04\n 5 1.00e- 8   0.287 rmse    standard   0.799     5 0.00167 Preprocessor1_Model03\n 6 3.44e- 4   0.125 rmse    standard   0.799     5 0.00167 Preprocessor1_Model01\n 7 2.35e-10   0.559 rmse    standard   0.799     5 0.00164 Preprocessor1_Model06\n 8 1.43e- 3   0.168 rmse    standard   0.799     5 0.00165 Preprocessor1_Model02\n 9 2.42e- 2   0.889 rmse    standard   0.802     5 0.00161 Preprocessor1_Model09\n10 2.36e- 1   0.990 rmse    standard   0.840     5 0.00206 Preprocessor1_Model10\n\n\nFind the best parameters of the group - finalize_workflow() will choose the model with the optimal set of hyperparameters as found in select_best().\n\n# Select the best hyperparameters based on RMSE\nbest_glm &lt;- select_best(glm_tune_results, metric = 'rmse')\n\n# Finalize the workflow with the best hyperparameters\nfinal_glm_workflow &lt;- finalize_workflow(glm_wflow, best_glm)\n\nbest_glm\n\n# A tibble: 1 × 3\n      penalty mixture .config              \n        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.000000838   0.469 Preprocessor1_Model05\n\n\n^ Above you’ll find the optimal configuration of the model. We won’t get too far into the weeds here, but will note that the penalty term is small (low regularization, high complexity) and the mixture (type of regularization) indicates more of a Ridge regression (more shrinkage, less variable elimination).\nBelow, we’ll take the best model and fit it to the entire training data set before validating it against the test set.\n\n# Fit the finalized model on the entire training data\nfinal_glm_fit &lt;- fit(final_glm_workflow, data = train_data)"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#cross-validation-tuning",
    "href": "posts/post-with-code/wr_fantasy_pts.html#cross-validation-tuning",
    "title": "Predicting Wide Receiver Fantasy Points w/ Tidymodels",
    "section": "Cross-Validation + Tuning",
    "text": "Cross-Validation + Tuning\n\nGLM\nWe’ll train a general linear model (GLM). This is a type of linear regression which provides built-in variable selection through a process called regularization. The nuts and bolts of this modeling strategy are beyond the scope of this post, but for those interested in learning, there are tons of material on this concept anywhere on the internet. Here’s a good reference from UW professor Shuai Huang.\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\nglm_spec &lt;- linear_reg(\n  penalty = tune(),     \n  mixture = tune(),    # Mixutre (0 = Ridge, 1 = Lasso, values in between = Elastic Net)\n    \n) %&gt;%\n  set_engine(\"glmnet\")\n\n# Setup workflow\nglm_wflow &lt;-\n  workflow() %&gt;% \n  add_recipe(exp_recipe) %&gt;%\n  add_model(glm_spec)\n\n# Create cross validation splits\nwr_folds &lt;- vfold_cv(train_data, v = 5)\n\n# Tune the hyperparameters using a grid of values\nglm_tune_results &lt;- tune_grid(\n  glm_wflow,\n  resamples = wr_folds,\n  grid = 10   # Number of tuning combinations to evaluate\n)\n\nExamine Tuning Results\n\n# Display tuning results\n\nglm_tune_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  arrange(mean)\n\n# A tibble: 10 × 8\n    penalty mixture .metric .estimator  mean     n std_err .config              \n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1 8.38e- 7   0.469 rmse    standard   0.799     5 0.00167 Preprocessor1_Model05\n 2 1.44e- 5   0.744 rmse    standard   0.799     5 0.00165 Preprocessor1_Model08\n 3 4.65e- 6   0.670 rmse    standard   0.799     5 0.00164 Preprocessor1_Model07\n 4 1.18e- 9   0.352 rmse    standard   0.799     5 0.00167 Preprocessor1_Model04\n 5 1.00e- 8   0.287 rmse    standard   0.799     5 0.00167 Preprocessor1_Model03\n 6 3.44e- 4   0.125 rmse    standard   0.799     5 0.00167 Preprocessor1_Model01\n 7 2.35e-10   0.559 rmse    standard   0.799     5 0.00164 Preprocessor1_Model06\n 8 1.43e- 3   0.168 rmse    standard   0.799     5 0.00165 Preprocessor1_Model02\n 9 2.42e- 2   0.889 rmse    standard   0.802     5 0.00161 Preprocessor1_Model09\n10 2.36e- 1   0.990 rmse    standard   0.840     5 0.00206 Preprocessor1_Model10\n\n\nFind the best parameters of the group - finalize_workflow() will choose the model with the optimal set of hyperparameters as found in select_best().\n\n# Select the best hyperparameters based on RMSE\nbest_glm &lt;- select_best(glm_tune_results, metric = 'rmse')\n\n# Finalize the workflow with the best hyperparameters\nfinal_glm_workflow &lt;- finalize_workflow(glm_wflow, best_glm)\n\nbest_glm\n\n# A tibble: 1 × 3\n      penalty mixture .config              \n        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.000000838   0.469 Preprocessor1_Model05\n\n\n^ Above you’ll find the optimal configuration of the model. We won’t get too far into the weeds here, but will note that the penalty term is small (low regularization, high complexity) and the mixture (type of regularization) indicates more of a Ridge regression (more shrinkage, less variable elimination).\nBelow, we’ll take the best model and fit it to the entire training data set before validating it against the test set.\n\n# Fit the finalized model on the entire training data\nfinal_glm_fit &lt;- fit(final_glm_workflow, data = train_data)"
  },
  {
    "objectID": "posts/nfl-game-prediction/game_pred2.html",
    "href": "posts/nfl-game-prediction/game_pred2.html",
    "title": "Predicting NFL Game Outcomes in R (part 2)",
    "section": "",
    "text": "Hello everyone! In the first post I shared on modeling NFL game outcomes, I shared a simple way to create a NFL game prediction model using nflreadR and tidymodels. In this post, I’ll share an easy way to tune our model.\nI’ll skip the data cleaning portion since it’s the same as the previous post. Ideally, I’d package these steps into a function as part of a larger package to spare you the time and make this process neater, repeatable, and editable. However, I’m not feeling very generous today (aka I’m too lazy), so I’ve copied and pasted the setup code in the hidden cell below.\n\n\nCode\nlibrary('nflreadr')\nlibrary(\"tidyverse\")\nlibrary('pROC')\nlibrary('tidymodels')\n\n# Scrape schedule Results\nload_schedules(seasons = seq(2011,2024)) -&gt; nfl_game_results \n\nnfl_game_results %&gt;%\n  # Remove the upcoming season\n  pivot_longer(cols = c(away_team,home_team),\n               names_to = \"home_away\",\n               values_to = \"team\") %&gt;%\n  mutate(team_score = ifelse(home_away == \"home_team\",yes = home_score,no = away_score),\n         opp_score = ifelse(home_away == \"home_team\", away_score,home_score)) %&gt;%  # sort for cumulative avg\n  arrange(season,week) %&gt;%\n  select(season,game_id,team,team_score,opp_score,week) -&gt; team_games\n\nteam_games %&gt;%\n  arrange(week) %&gt;%\n  group_by(season,team) %&gt;%\n  # For each team's season calculate the cumulative scores for after each week\n  mutate(cumul_score_mean = cummean(team_score),\n          cumul_score_opp = cummean(opp_score),\n          cumul_wins = cumsum(team_score &gt; opp_score),\n          cumul_losses = cumsum(team_score &lt; opp_score),\n          cumul_ties = cumsum(team_score == opp_score),\n         cumul_win_pct = cumul_wins / (cumul_wins + cumul_losses),\n         # Create the lag variable\n         cumul_win_pct_lag_1 = lag(cumul_win_pct,1),\n         cumul_score_lag_1 = lag(cumul_score_mean,1),\n         cumul_opp_lag_1 = lag(cumul_score_opp,1)\n         ) %&gt;%\n  # Non-lag variables leak info\n  select(week,game_id,contains('lag_1')) %&gt;%\n  ungroup() -&gt; cumul_avgs\n\n# Calculate average win pct.\nteam_games %&gt;%\n  group_by(season,team) %&gt;%\n  summarise(wins = sum(team_score &gt; opp_score),\n            losses = sum(team_score &lt; opp_score),\n            ties = sum(team_score == opp_score))%&gt;%\n  ungroup() %&gt;%\n  arrange(season) %&gt;%\n  group_by(team) %&gt;%\n  mutate(win_pct = wins / (wins + losses),\n         lag1_win_pct = lag(win_pct,1)) %&gt;%\n  ungroup() -&gt; team_win_pct\n\n# Load depth charts and injury reports\ndc = load_depth_charts(seq(2011,most_recent_season()))\ninjuries = load_injuries(seq(2011,most_recent_season()))\n\ninjuries %&gt;%\n  filter(report_status == \"Out\") -&gt; out_inj\n\ndc %&gt;% \n  filter(depth_team == 1) -&gt; starters\n\n# Determine roster position of injured players\nstarters %&gt;%\n  select(-c(last_name,first_name,position,full_name)) %&gt;%\n  inner_join(out_inj, by = c('season','club_code' = 'team','gsis_id','game_type','week')) -&gt; injured_starters\n\n# Number of injuries by position\ninjured_starters %&gt;%\n  group_by(season,club_code,week,position) %&gt;%\n  summarise(starters_injured = n()) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from = position, names_prefix = \"injured_\",values_from = starters_injured) -&gt; injuries_position\n\nnfl_game_results %&gt;%\n  inner_join(cumul_avgs, by = c('game_id','season','week','home_team' = 'team')) %&gt;%\n  inner_join(cumul_avgs, by = c('game_id','season','week','away_team' = 'team'),suffix = c('_home','_away'))-&gt; w_avgs\n\n# Check for stragglers\nnfl_game_results %&gt;%\n  anti_join(cumul_avgs, by = c('game_id','season','home_team' = 'team','week')) -&gt; unplayed_games\n\n\n# Indicate whether home team won\nw_avgs %&gt;%\n  mutate(home_win = as.numeric(result &gt; 0)) -&gt; matchups\n\nmatchups %&gt;%\n  left_join(injuries_position,by = c('season','home_team'='club_code','week')) %&gt;%\n  left_join(injuries_position,by = c('season','away_team'='club_code','week'),suffix = c('_home','_away')) %&gt;%\n  mutate(across(starts_with('injured_'), ~replace_na(.x, 0))) -&gt; matchup_full\n\n# Remove unneeded columns\nmatchup_full %&gt;%\n  select(-c(home_score,away_score,overtime,home_team,away_team,away_qb_name,home_qb_name,referee,stadium,home_coach,away_coach,ftn,espn,old_game_id,gsis,nfl_detail_id,pfr,pff,result)) -&gt; matchup_ready\n\n\n# Remove columns\nmatchup_ready = matchup_ready%&gt;%\n  # Transform outcome into factor variable\n  select(where(is.numeric),game_id) %&gt;% \n  mutate(home_win = as.factor(home_win)) \n\n\nLong story short, I want each row in the final dataset to represent an NFL game with each team’s :\n\nPrevious performance\nInjuries\nOpponent’s previous performance\nOpponent’s injuries\n\nHere’s what our data looks like …\n\nhead(matchup_ready)\n\n# A tibble: 6 × 56\n  season  week total away_rest home_rest away_moneyline home_moneyline\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;          &lt;int&gt;          &lt;int&gt;\n1   2011     1    76         7         7            222           -250\n2   2011     1    42         7         7            113           -125\n3   2011     1    42         7         7           -116            105\n4   2011     1    44         7         7            273           -310\n5   2011     1    41         7         7            369           -430\n6   2011     1    30         7         7           -108           -102\n# ℹ 49 more variables: spread_line &lt;dbl&gt;, away_spread_odds &lt;int&gt;,\n#   home_spread_odds &lt;int&gt;, total_line &lt;dbl&gt;, under_odds &lt;int&gt;,\n#   over_odds &lt;int&gt;, div_game &lt;int&gt;, temp &lt;int&gt;, wind &lt;int&gt;,\n#   cumul_win_pct_lag_1_home &lt;dbl&gt;, cumul_score_lag_1_home &lt;dbl&gt;,\n#   cumul_opp_lag_1_home &lt;dbl&gt;, cumul_win_pct_lag_1_away &lt;dbl&gt;,\n#   cumul_score_lag_1_away &lt;dbl&gt;, cumul_opp_lag_1_away &lt;dbl&gt;, home_win &lt;fct&gt;,\n#   injured_S_home &lt;int&gt;, injured_LB_home &lt;int&gt;, injured_RB_home &lt;int&gt;, …\n\n\nThere are certainly better ways to represent these pieces of information than what I’ve managed to do above (please let me know!), but this post will only focus on the simple step of model tuning.\nWith that, I’ll start directly from the modeling portion of the code. In the sections below, you’ll see our basic tidymodels setup using our recipes object for pre-processing.\n\nlibrary('tidymodels')\n\n# Split Data\nset.seed(123)\nmatchups24 = matchup_ready %&gt;% filter(season == 2024)\nsplits = matchup_ready %&gt;% \n  filter(season != 2024) %&gt;%\n  initial_split(prop = 0.7)\ntrain_data &lt;- training(splits)\ntest_data  &lt;- testing(splits)\n\n\nrec_impute = recipe(formula = home_win ~ .,\n                 data = train_data) %&gt;%\n  #create ID role (do not remove) game ID. We'll use this to match predictions to specific games\n  update_role(game_id, new_role = \"ID\")%&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  #for each numeric variable/feature, replace any NA's with the median value of that variable/feature\n  step_impute_median(all_numeric_predictors()) \n\nWe added an extra step of removing zero-variance predictors.\n\nimp_models &lt;- rec_impute %&gt;%\n  check_missing(all_numeric_predictors()) %&gt;%\n  prep(training = train_data)\n\n# Check the predictors that were filtered out\nnzv_step &lt;- imp_models$steps[[2]]  # Access the step_nzv object\nremoved_predictors &lt;- nzv_step$removals\n\n# Display removed predictors\nremoved_predictors\n\nNULL\n\n# Check if imputation worked\nimp_models %&gt;%\n  bake(train_data) %&gt;%\n  is.na() %&gt;%\n  colSums()\n\n                  season                     week                    total \n                       0                        0                        0 \n               away_rest                home_rest           away_moneyline \n                       0                        0                        0 \n          home_moneyline              spread_line         away_spread_odds \n                       0                        0                        0 \n        home_spread_odds               total_line               under_odds \n                       0                        0                        0 \n               over_odds                 div_game                     temp \n                       0                        0                        0 \n                    wind cumul_win_pct_lag_1_home   cumul_score_lag_1_home \n                       0                        0                        0 \n    cumul_opp_lag_1_home cumul_win_pct_lag_1_away   cumul_score_lag_1_away \n                       0                        0                        0 \n    cumul_opp_lag_1_away           injured_S_home          injured_LB_home \n                       0                        0                        0 \n         injured_RB_home           injured_T_home           injured_C_home \n                       0                        0                        0 \n         injured_DT_home          injured_WR_home          injured_CB_home \n                       0                        0                        0 \n          injured_G_home           injured_K_home          injured_TE_home \n                       0                        0                        0 \n         injured_QB_home          injured_DE_home          injured_LS_home \n                       0                        0                        0 \n          injured_P_home          injured_FB_home           injured_S_away \n                       0                        0                        0 \n         injured_LB_away          injured_RB_away           injured_T_away \n                       0                        0                        0 \n          injured_C_away          injured_DT_away          injured_WR_away \n                       0                        0                        0 \n         injured_CB_away           injured_G_away           injured_K_away \n                       0                        0                        0 \n         injured_TE_away          injured_QB_away          injured_DE_away \n                       0                        0                        0 \n         injured_LS_away           injured_P_away          injured_FB_away \n                       0                        0                        0 \n                 game_id                 home_win \n                       0                        0"
  },
  {
    "objectID": "posts/nfl-game-prediction/game_pred2.html#intro",
    "href": "posts/nfl-game-prediction/game_pred2.html#intro",
    "title": "Predicting NFL Game Outcomes in R (part 2)",
    "section": "",
    "text": "Hello everyone! In the first post I shared on modeling NFL game outcomes, I shared a simple way to create a NFL game prediction model using nflreadR and tidymodels. In this post, I’ll share an easy way to tune our model.\nI’ll skip the data cleaning portion since it’s the same as the previous post. Ideally, I’d package these steps into a function as part of a larger package to spare you the time and make this process neater, repeatable, and editable. However, I’m not feeling very generous today (aka I’m too lazy), so I’ve copied and pasted the setup code in the hidden cell below.\n\n\nCode\nlibrary('nflreadr')\nlibrary(\"tidyverse\")\nlibrary('pROC')\nlibrary('tidymodels')\n\n# Scrape schedule Results\nload_schedules(seasons = seq(2011,2024)) -&gt; nfl_game_results \n\nnfl_game_results %&gt;%\n  # Remove the upcoming season\n  pivot_longer(cols = c(away_team,home_team),\n               names_to = \"home_away\",\n               values_to = \"team\") %&gt;%\n  mutate(team_score = ifelse(home_away == \"home_team\",yes = home_score,no = away_score),\n         opp_score = ifelse(home_away == \"home_team\", away_score,home_score)) %&gt;%  # sort for cumulative avg\n  arrange(season,week) %&gt;%\n  select(season,game_id,team,team_score,opp_score,week) -&gt; team_games\n\nteam_games %&gt;%\n  arrange(week) %&gt;%\n  group_by(season,team) %&gt;%\n  # For each team's season calculate the cumulative scores for after each week\n  mutate(cumul_score_mean = cummean(team_score),\n          cumul_score_opp = cummean(opp_score),\n          cumul_wins = cumsum(team_score &gt; opp_score),\n          cumul_losses = cumsum(team_score &lt; opp_score),\n          cumul_ties = cumsum(team_score == opp_score),\n         cumul_win_pct = cumul_wins / (cumul_wins + cumul_losses),\n         # Create the lag variable\n         cumul_win_pct_lag_1 = lag(cumul_win_pct,1),\n         cumul_score_lag_1 = lag(cumul_score_mean,1),\n         cumul_opp_lag_1 = lag(cumul_score_opp,1)\n         ) %&gt;%\n  # Non-lag variables leak info\n  select(week,game_id,contains('lag_1')) %&gt;%\n  ungroup() -&gt; cumul_avgs\n\n# Calculate average win pct.\nteam_games %&gt;%\n  group_by(season,team) %&gt;%\n  summarise(wins = sum(team_score &gt; opp_score),\n            losses = sum(team_score &lt; opp_score),\n            ties = sum(team_score == opp_score))%&gt;%\n  ungroup() %&gt;%\n  arrange(season) %&gt;%\n  group_by(team) %&gt;%\n  mutate(win_pct = wins / (wins + losses),\n         lag1_win_pct = lag(win_pct,1)) %&gt;%\n  ungroup() -&gt; team_win_pct\n\n# Load depth charts and injury reports\ndc = load_depth_charts(seq(2011,most_recent_season()))\ninjuries = load_injuries(seq(2011,most_recent_season()))\n\ninjuries %&gt;%\n  filter(report_status == \"Out\") -&gt; out_inj\n\ndc %&gt;% \n  filter(depth_team == 1) -&gt; starters\n\n# Determine roster position of injured players\nstarters %&gt;%\n  select(-c(last_name,first_name,position,full_name)) %&gt;%\n  inner_join(out_inj, by = c('season','club_code' = 'team','gsis_id','game_type','week')) -&gt; injured_starters\n\n# Number of injuries by position\ninjured_starters %&gt;%\n  group_by(season,club_code,week,position) %&gt;%\n  summarise(starters_injured = n()) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from = position, names_prefix = \"injured_\",values_from = starters_injured) -&gt; injuries_position\n\nnfl_game_results %&gt;%\n  inner_join(cumul_avgs, by = c('game_id','season','week','home_team' = 'team')) %&gt;%\n  inner_join(cumul_avgs, by = c('game_id','season','week','away_team' = 'team'),suffix = c('_home','_away'))-&gt; w_avgs\n\n# Check for stragglers\nnfl_game_results %&gt;%\n  anti_join(cumul_avgs, by = c('game_id','season','home_team' = 'team','week')) -&gt; unplayed_games\n\n\n# Indicate whether home team won\nw_avgs %&gt;%\n  mutate(home_win = as.numeric(result &gt; 0)) -&gt; matchups\n\nmatchups %&gt;%\n  left_join(injuries_position,by = c('season','home_team'='club_code','week')) %&gt;%\n  left_join(injuries_position,by = c('season','away_team'='club_code','week'),suffix = c('_home','_away')) %&gt;%\n  mutate(across(starts_with('injured_'), ~replace_na(.x, 0))) -&gt; matchup_full\n\n# Remove unneeded columns\nmatchup_full %&gt;%\n  select(-c(home_score,away_score,overtime,home_team,away_team,away_qb_name,home_qb_name,referee,stadium,home_coach,away_coach,ftn,espn,old_game_id,gsis,nfl_detail_id,pfr,pff,result)) -&gt; matchup_ready\n\n\n# Remove columns\nmatchup_ready = matchup_ready%&gt;%\n  # Transform outcome into factor variable\n  select(where(is.numeric),game_id) %&gt;% \n  mutate(home_win = as.factor(home_win)) \n\n\nLong story short, I want each row in the final dataset to represent an NFL game with each team’s :\n\nPrevious performance\nInjuries\nOpponent’s previous performance\nOpponent’s injuries\n\nHere’s what our data looks like …\n\nhead(matchup_ready)\n\n# A tibble: 6 × 56\n  season  week total away_rest home_rest away_moneyline home_moneyline\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;          &lt;int&gt;          &lt;int&gt;\n1   2011     1    76         7         7            222           -250\n2   2011     1    42         7         7            113           -125\n3   2011     1    42         7         7           -116            105\n4   2011     1    44         7         7            273           -310\n5   2011     1    41         7         7            369           -430\n6   2011     1    30         7         7           -108           -102\n# ℹ 49 more variables: spread_line &lt;dbl&gt;, away_spread_odds &lt;int&gt;,\n#   home_spread_odds &lt;int&gt;, total_line &lt;dbl&gt;, under_odds &lt;int&gt;,\n#   over_odds &lt;int&gt;, div_game &lt;int&gt;, temp &lt;int&gt;, wind &lt;int&gt;,\n#   cumul_win_pct_lag_1_home &lt;dbl&gt;, cumul_score_lag_1_home &lt;dbl&gt;,\n#   cumul_opp_lag_1_home &lt;dbl&gt;, cumul_win_pct_lag_1_away &lt;dbl&gt;,\n#   cumul_score_lag_1_away &lt;dbl&gt;, cumul_opp_lag_1_away &lt;dbl&gt;, home_win &lt;fct&gt;,\n#   injured_S_home &lt;int&gt;, injured_LB_home &lt;int&gt;, injured_RB_home &lt;int&gt;, …\n\n\nThere are certainly better ways to represent these pieces of information than what I’ve managed to do above (please let me know!), but this post will only focus on the simple step of model tuning.\nWith that, I’ll start directly from the modeling portion of the code. In the sections below, you’ll see our basic tidymodels setup using our recipes object for pre-processing.\n\nlibrary('tidymodels')\n\n# Split Data\nset.seed(123)\nmatchups24 = matchup_ready %&gt;% filter(season == 2024)\nsplits = matchup_ready %&gt;% \n  filter(season != 2024) %&gt;%\n  initial_split(prop = 0.7)\ntrain_data &lt;- training(splits)\ntest_data  &lt;- testing(splits)\n\n\nrec_impute = recipe(formula = home_win ~ .,\n                 data = train_data) %&gt;%\n  #create ID role (do not remove) game ID. We'll use this to match predictions to specific games\n  update_role(game_id, new_role = \"ID\")%&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  #for each numeric variable/feature, replace any NA's with the median value of that variable/feature\n  step_impute_median(all_numeric_predictors()) \n\nWe added an extra step of removing zero-variance predictors.\n\nimp_models &lt;- rec_impute %&gt;%\n  check_missing(all_numeric_predictors()) %&gt;%\n  prep(training = train_data)\n\n# Check the predictors that were filtered out\nnzv_step &lt;- imp_models$steps[[2]]  # Access the step_nzv object\nremoved_predictors &lt;- nzv_step$removals\n\n# Display removed predictors\nremoved_predictors\n\nNULL\n\n# Check if imputation worked\nimp_models %&gt;%\n  bake(train_data) %&gt;%\n  is.na() %&gt;%\n  colSums()\n\n                  season                     week                    total \n                       0                        0                        0 \n               away_rest                home_rest           away_moneyline \n                       0                        0                        0 \n          home_moneyline              spread_line         away_spread_odds \n                       0                        0                        0 \n        home_spread_odds               total_line               under_odds \n                       0                        0                        0 \n               over_odds                 div_game                     temp \n                       0                        0                        0 \n                    wind cumul_win_pct_lag_1_home   cumul_score_lag_1_home \n                       0                        0                        0 \n    cumul_opp_lag_1_home cumul_win_pct_lag_1_away   cumul_score_lag_1_away \n                       0                        0                        0 \n    cumul_opp_lag_1_away           injured_S_home          injured_LB_home \n                       0                        0                        0 \n         injured_RB_home           injured_T_home           injured_C_home \n                       0                        0                        0 \n         injured_DT_home          injured_WR_home          injured_CB_home \n                       0                        0                        0 \n          injured_G_home           injured_K_home          injured_TE_home \n                       0                        0                        0 \n         injured_QB_home          injured_DE_home          injured_LS_home \n                       0                        0                        0 \n          injured_P_home          injured_FB_home           injured_S_away \n                       0                        0                        0 \n         injured_LB_away          injured_RB_away           injured_T_away \n                       0                        0                        0 \n          injured_C_away          injured_DT_away          injured_WR_away \n                       0                        0                        0 \n         injured_CB_away           injured_G_away           injured_K_away \n                       0                        0                        0 \n         injured_TE_away          injured_QB_away          injured_DE_away \n                       0                        0                        0 \n         injured_LS_away           injured_P_away          injured_FB_away \n                       0                        0                        0 \n                 game_id                 home_win \n                       0                        0"
  },
  {
    "objectID": "posts/nfl-game-prediction/game_pred2.html#modeling",
    "href": "posts/nfl-game-prediction/game_pred2.html#modeling",
    "title": "Predicting NFL Game Outcomes in R (part 2)",
    "section": "Modeling",
    "text": "Modeling\nAlso left out in the last post was the crucial step of tuning (or optimizing) those regularization parameters. We fit our model using easy handpicked hand-picked values of mixture = 0.05 and penalty = 1. This time, we’ll try to tune our logistic regression to find the optimal values for both mixture and penalty based on model performance and check if that results in better (and more trustworthy) prediction accuracy. To do that, we introduce two new lines, penalty = tune() and mixture = tune() in the glm_spec variable.\n\nlibrary('glmnet')\n# Penalized Linear Regression\n\n\nglm_spec &lt;- logistic_reg(\n  penalty = tune(),     \n  mixture = tune()   \n) %&gt;%\n  set_engine(\"glmnet\")\n\n# Setup workflow\nglm_wflow &lt;-\n  workflow() %&gt;% \n  add_recipe(rec_impute) %&gt;%\n  add_model(glm_spec)\n\n# Create cross validation splits\nfolds &lt;- vfold_cv(train_data, v = 5)\n\n# Tune the hyperparameters using a grid of values\nglm_tune_results &lt;- tune_grid(\n  glm_wflow,\n  resamples = folds,\n  metrics = metric_set(brier_class),\n  grid = 10   # Number of tuning combinations to evaluate\n)\n\nThese 2 lines will incorporate the tuning/optimization into our workflow. To finish the job, we add one more line metrics = metric_set(brier_class) as the evaluation criteria we’re attempting to optimize for. Why we are using brier_class() is explained below."
  },
  {
    "objectID": "posts/nfl-game-prediction/game_pred2.html#regularization",
    "href": "posts/nfl-game-prediction/game_pred2.html#regularization",
    "title": "Predicting NFL Game Outcomes in R (part 2)",
    "section": "Regularization",
    "text": "Regularization\nIn the last post, we introduced logistic regression as our model of choice. We wanted (and still want) to predict the probability that the home team wins a matchup and do that, we need a classification model.\nEnter logistic regression, the simple, yet powerful classification technique made for classifying linear data.\nIn logistic regression, we have the option of introducing something called regularization, which is just a fancy word for telling the model “don’t overcomplicate things!”. As you saw in the feature enigneering steps in the last post, there are a lot of variables and in many real world models (AI models included) the number of parameters can be in the billions! This is often too much information to pare down, so regularization is a simple step to help the model focus on the most important stuff. This extra step helps the model avoid a common ML pitfall - overfitting. Think of it like this, imagine you have a math test tomorrow and your friend sends you the solutions to the practice exam. You study by memorizing the answers to practce exam (not by DOING the problems). The next day to open your test and … shoot… you have to SHOW your work?? You don’t even know where to start. That is overfitting, you’ve memorized the answers, but you don’t know HOW to solve the problems or complete the process.\nRegularization is a way to penalize the model from memorizing inputs and instead forces it to make generalizations about relationships in the data (aka learn general patterns). You sacrifice a bit accuracy, for example, maybe you mix up some calculations on the exam incorrect, but you get partial credit for showing proper process. There are some other pros and cons to regularization, but I’ll leave that for you to research yourself.\n\nlibrary('glmnet')\n# Penalized Linear Regression\n\n\nglm_spec &lt;- logistic_reg(\n  penalty = tune(),     \n  mixture = tune(),    # Mixutre (0 = Ridge, 1 = Lasso, values in between = Elastic Net)\n    \n) %&gt;%\n  set_engine(\"glmnet\")\n\n# Setup workflow\nglm_wflow &lt;-\n  workflow() %&gt;% \n  add_recipe(rec_impute) %&gt;%\n  add_model(glm_spec)\n\n# Create cross validation splits\nfolds &lt;- vfold_cv(train_data, v = 5)\n\n# Tune the hyperparameters using a grid of values\nglm_tune_results &lt;- tune_grid(\n  glm_wflow,\n  resamples = folds,\n  grid = 10   # Number of tuning combinations to evaluate\n)\n\nNext, let’s take a look at the results. We’re primarily concerned with brier score since we want to measure how well our predicted probabilies are calibrated to actual results. Calibration refers to how well the predicted probabilities from a model match the actual observed outcomes. A well-calibrated model means that if it predicts a 70% chance of an event happening, the event should occur about 70% of the time in reality. For example, if a model predicts a team wins 60% of the time, the team should win about 60% of the time.\nBrier score is a representation of that calibration on a 0-1 scale. The lower the score, the better Seeing a slight reduction in brier score between the last attempt (0.229) and this attempt (0.214) is good! If we want to go the extra mile, we can test the significance of the difference across CV folds to determine if this reduction is significant.\n\n# Display tuning results\nglm_tune_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"brier_class\") %&gt;%\n  arrange(mean) %&gt;%\n  slice(1)\n\n# A tibble: 1 × 8\n  penalty mixture .metric     .estimator  mean     n std_err .config            \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;              \n1  0.0208   0.185 brier_class binary     0.215     5 0.00432 Preprocessor1_Mode…\n\n\nLooks like the combo of scaling, cross-validation lead us to better model calibration!\n\n# Select the best hyperparameters based on RMSE\nbest_glm &lt;- select_best(glm_tune_results, metric = 'brier_class')\n\n# Finalize the workflow with the best hyperparameters\nfinal_glm_workflow &lt;- finalize_workflow(glm_wflow, best_glm)\n\nbest_glm\n\n# A tibble: 1 × 3\n  penalty mixture .config              \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1  0.0208   0.185 Preprocessor1_Model02\n\n\n\n# Fit the finalized model on the entire training data\nfinal_glm_fit &lt;- fit(final_glm_workflow, data =train_data)\n\nLet’s try out our new evaluation metric to check how well our model is calibrated with the actual output. The graph below is calibration plot. It tells us how aligned our predictions are with the actual observed outcomes. The x-axis is straightforward - it represents our predicted probabilities. The Y-Axis, however, is the proportion of positive observations. This displays our home_win outcome variable as a proportion so we can compare on the scale of the predicted probability class.\n\n# Align predictions to test dataset\npredicted_df = augment(final_glm_fit,test_data) \npredicted_df |&gt; \n  mutate(bin = cut(.pred_1, breaks = seq(0, 1, by = 0.1), include.lowest = TRUE)) %&gt;%\n  group_by(bin) %&gt;%\n  summarise(\n    avg_predicted = mean(.pred_1),\n    observed_proportion = mean(as.numeric(home_win)-1,na.rm = T),\n    n = n()\n  ) |&gt; \n  ggplot(aes(x = avg_predicted, y = observed_proportion)) +\n  geom_point() +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Calibration Plot\",\n    x = \"Mean Predicted Probability\",\n    y = \"Observed Proportion\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt looks like in this case, the model is better calibrated (in-line with actual results) for games between a 50-70% win pct for the home team. This doesn’t tell us much other than that the model is unreliable on the extreme margins, however, NFL games rarely have games with such extreme dogs/favorites.\nRegularization and model tuning is just one way to improve model perforance. In the next post, we’ll likely take another crack at improving this model. Thank you for reading and feel free to reach out to me with any questions/suggestions!"
  },
  {
    "objectID": "posts/nfl-game-prediction/game_pred2.html#logistic-regression-and-regularization",
    "href": "posts/nfl-game-prediction/game_pred2.html#logistic-regression-and-regularization",
    "title": "Predicting NFL Game Outcomes in R (part 2)",
    "section": "Logistic Regression and Regularization",
    "text": "Logistic Regression and Regularization\nIn the last post, we introduced logistic regression as our model of choice. We wanted (and still want) to predict the probability that the home team wins a matchup and do that, we need a classification model.\nEnter logistic regression, the simple, yet powerful classification technique made for classifying linear data.\nIn logistic regression, we have the option of introducing something called regularization, which is just a fancy word for telling the model “don’t overcomplicate things!”. As you saw in the feature enigneering steps in the last post, there are a lot of variables and in many real world models (AI models included) the number of parameters can be in the billions! This is often too much information to pare down, so regularization is a simple step to help the model focus on the most important stuff. This extra step helps the model avoid a common ML pitfall - overfitting. Think of it like this, imagine you have a math test tomorrow and your friend sends you the solutions to the practice exam. You study by memorizing the answers to practce exam (not by DOING the problems). The next day to open your test and … shoot… you have to SHOW your work?? You don’t even know where to start. That is overfitting, you’ve memorized the answers, but you don’t know HOW to solve the problems or complete the process.\nRegularization is a way to penalize the model from memorizing inputs and instead forces it to make generalizations about relationships in the data (aka learn general patterns). You sacrifice a bit accuracy, for example, maybe you mix up some calculations on the exam incorrect, but you get partial credit for showing proper process. There are some other pros and cons to regularization, but I’ll leave that for you to research yourself."
  },
  {
    "objectID": "posts/nfl-game-prediction/game_pred2.html#brier-score",
    "href": "posts/nfl-game-prediction/game_pred2.html#brier-score",
    "title": "Predicting NFL Game Outcomes in R (part 2)",
    "section": "Brier Score",
    "text": "Brier Score\nNext, let’s take a look at the results. We’re primarily concerned with brier score since we want to measure how well our predicted probabilities are calibrated to actual results. Calibration refers to how well the predicted probabilities from a model match the actual observed outcomes. A well-calibrated model means that if it predicts a 70% chance of an event happening, the event should occur about 70% of the time in reality. For example, if a model predicts a team wins 60% of the time, the team should win about 60% of the time.\nBrier score is a representation of that calibration on a 0-1 scale. The lower the score, the better Seeing a slight reduction in brier score between the last attempt (0.229) and this attempt (0.216) is good! If we want to go the extra mile, we can test the significance of the difference across CV folds to determine if this reduction is significant.\n\n# Display tuning results\nglm_tune_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"brier_class\") %&gt;%\n  arrange(mean) %&gt;%\n  slice(1)\n\n# A tibble: 1 × 8\n  penalty mixture .metric     .estimator  mean     n std_err .config            \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;              \n1  0.0208   0.185 brier_class binary     0.215     5 0.00432 Preprocessor1_Mode…\n\n\nLooks like the tuning lead us to better model calibration!\n\n# Select the best hyperparameters based on RMSE\nbest_glm &lt;- select_best(glm_tune_results, metric = 'brier_class')\n\n# Finalize the workflow with the best hyperparameters\nfinal_glm_workflow &lt;- finalize_workflow(glm_wflow, best_glm)\n\nbest_glm\n\n# A tibble: 1 × 3\n  penalty mixture .config              \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1  0.0208   0.185 Preprocessor1_Model02\n\n\n\n# Fit the finalized model on the entire training data\nfinal_glm_fit &lt;- fit(final_glm_workflow, data =train_data)\n\nLet’s try out our new evaluation metric to check how well our model is calibrated with the actual output. The graph below is calibration plot. It tells us how aligned our predictions are with the actual observed outcomes. The x-axis is straightforward - it represents our predicted probabilities. The Y-Axis, however, is the proportion of positive observations. This displays our home_win outcome variable as a proportion so we can compare on the scale of the predicted probability class.\n\n# Align predictions to test dataset\npredicted_df = augment(final_glm_fit,test_data) \npredicted_df |&gt; \n  mutate(bin = cut(.pred_1, breaks = seq(0, 1, by = 0.1), include.lowest = TRUE)) %&gt;%\n  group_by(bin) %&gt;%\n  summarise(\n    avg_predicted = mean(.pred_1),\n    observed_proportion = mean(as.numeric(home_win)-1,na.rm = T),\n    n = n()\n  ) |&gt; \n  ggplot(aes(x = avg_predicted, y = observed_proportion)) +\n  geom_point() +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Calibration Plot\",\n    x = \"Mean Predicted Probability\",\n    y = \"Observed Proportion\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt looks like in this case, the model is better calibrated (in-line with actual results) for games between a 50-70% win pct for the home team. This doesn’t tell us much other than that the model is unreliable on the extreme margins, however, NFL games rarely have games with such extreme dogs/favorites.\nRegularization and model tuning is just one way to improve model perforance. In the next post, we’ll likely take another crack at improving this model. Thank you for reading and feel free to reach out to me with any questions/suggestions!"
  }
]